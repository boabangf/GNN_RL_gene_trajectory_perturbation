{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fff53b4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟡 Using CPU.\n",
      "=== Supervised training ImmuneNet (linear-only) ===\n",
      "[Supervised] Epoch 1 | Loss=0.416644 | MSE=0.2204 | R2=-2.294 | Pearson=0.143\n",
      "[Supervised] Epoch 2 | Loss=0.398956 | MSE=0.3443 | R2=-4.145 | Pearson=0.180\n",
      "[Test ImmuneNet] MSE=0.3533 | RMSE=0.5944 | MAE=0.4947 | R2=-4.001 | Pearson=0.148\n",
      "\n",
      "===== Mode: CONVEX =====\n",
      "[PPO/convex] Ep 1/2 Return=0.2883\n",
      "[PPO/convex] Ep 2/2 Return=0.2700\n",
      "[Distill/convex] Ep 01/10 loss=33.465093\n",
      "[Distill/convex] Ep 02/10 loss=24.684720\n",
      "[Distill/convex] Ep 03/10 loss=20.100252\n",
      "[Distill/convex] Ep 04/10 loss=16.113921\n",
      "[Distill/convex] Ep 05/10 loss=12.540359\n",
      "[Distill/convex] Ep 06/10 loss=9.232134\n",
      "[Distill/convex] Ep 07/10 loss=6.122596\n",
      "[Distill/convex] Ep 08/10 loss=3.245446\n",
      "[Distill/convex] Ep 09/10 loss=1.290944\n",
      "[Distill/convex] Ep 10/10 loss=0.435034\n",
      "[Dynamic/convex] Return=0.2796 | ΔCyt=0.1102 | Stability=13.405 | Corr(Reward,||C||)=0.017 | R2(Resp,Reward)=0.354 | MSE=0.1925 | RMSE=0.4384 | MAE=0.3434 | R2=-1.579 | Pearson=0.017\n",
      "\n",
      "===== Mode: NONCONVEX =====\n",
      "[PPO/nonconvex] Ep 1/2 Return=0.2817\n",
      "[PPO/nonconvex] Ep 2/2 Return=0.2772\n",
      "[Distill/nonconvex] Ep 01/10 loss=44.660292\n",
      "[Distill/nonconvex] Ep 02/10 loss=33.542307\n",
      "[Distill/nonconvex] Ep 03/10 loss=29.766730\n",
      "[Distill/nonconvex] Ep 04/10 loss=26.327516\n",
      "[Distill/nonconvex] Ep 05/10 loss=22.544148\n",
      "[Distill/nonconvex] Ep 06/10 loss=18.956618\n",
      "[Distill/nonconvex] Ep 07/10 loss=15.177374\n",
      "[Distill/nonconvex] Ep 08/10 loss=10.092563\n",
      "[Distill/nonconvex] Ep 09/10 loss=4.023939\n",
      "[Distill/nonconvex] Ep 10/10 loss=0.856995\n",
      "[Dynamic/nonconvex] Return=0.2837 | ΔCyt=0.1102 | Stability=11.813 | Corr(Reward,||C||)=-0.008 | R2(Resp,Reward)=0.362 | MSE=0.2336 | RMSE=0.4831 | MAE=0.3804 | R2=-1.755 | Pearson=-0.008\n",
      "\n",
      "===== Mode: TWOSTAGE =====\n",
      "[PPO/twostage] Ep 1/2 Return=0.2824\n",
      "[PPO/twostage] Ep 2/2 Return=0.2878\n",
      "[Distill/twostage] Ep 01/10 loss=19.911642\n",
      "[Distill/twostage] Ep 02/10 loss=13.242008\n",
      "[Distill/twostage] Ep 03/10 loss=10.415730\n",
      "[Distill/twostage] Ep 04/10 loss=8.090611\n",
      "[Distill/twostage] Ep 05/10 loss=5.891263\n",
      "[Distill/twostage] Ep 06/10 loss=3.822415\n",
      "[Distill/twostage] Ep 07/10 loss=2.074091\n",
      "[Distill/twostage] Ep 08/10 loss=0.883583\n",
      "[Distill/twostage] Ep 09/10 loss=0.307905\n",
      "[Distill/twostage] Ep 10/10 loss=0.126687\n",
      "[Dynamic/twostage] Return=0.2917 | ΔCyt=0.1158 | Stability=12.445 | Corr(Reward,||C||)=0.146 | R2(Resp,Reward)=0.374 | MSE=0.2096 | RMSE=0.4571 | MAE=0.3660 | R2=-1.608 | Pearson=0.146\n",
      "✅ Saved ppo_dynamic_cytokine_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Immune RL + PPO Distillation (Dynamic Cytokine Control Comparison)\n",
    "------------------------------------------------------------------\n",
    "- One frozen ImmuneNet encoder (linear-only)\n",
    "- PPO Teacher→Student distillation across modes: convex / nonconvex / twostage\n",
    "- Dynamic evaluation (Option 2): cytokine control behavior during rollouts\n",
    "- Saves: ppo_dynamic_cytokine_comparison.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, csv, argparse, random, warnings, math\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ============================================================\n",
    "# Torch setup\n",
    "# ============================================================\n",
    "HAS_TORCH = True\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader, Subset\n",
    "except Exception:\n",
    "    HAS_TORCH = False\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    if HAS_TORCH:\n",
    "        torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_device():\n",
    "    if not HAS_TORCH: return None\n",
    "    if torch.cuda.is_available():\n",
    "        d = torch.device(\"cuda\")\n",
    "        print(f\"🟢 Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "        return d\n",
    "    print(\"🟡 Using CPU.\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# ============================================================\n",
    "# Constants\n",
    "# ============================================================\n",
    "AA_VOCAB = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "AA_TO_ID = {a: i + 1 for i, a in enumerate(AA_VOCAB)}\n",
    "CYTOKINES = [\"NONE\", \"IL2\", \"IFNG\", \"IL10\", \"TNFA\"]\n",
    "CYTOKINE_TO_ID = {c: i for i, c in enumerate(CYTOKINES)}\n",
    "IDX_NONE = CYTOKINE_TO_ID[\"NONE\"]\n",
    "\n",
    "# ============================================================\n",
    "# Data utilities\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Example:\n",
    "    peptide: str\n",
    "    allele: str\n",
    "    score: float\n",
    "    tcr: Optional[str] = None\n",
    "\n",
    "def smart_read_table(path: str) -> List[List[str]]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", newline=\"\") as f:\n",
    "        sample = f.read(2048); f.seek(0)\n",
    "        import csv as _csv\n",
    "        try: dialect = _csv.Sniffer().sniff(sample, delimiters=\"\\t,;\")\n",
    "        except Exception:\n",
    "            class dialect: delimiter = \",\"\n",
    "        reader = _csv.reader(f, dialect)\n",
    "        for row in reader:\n",
    "            if row: rows.append([c.strip() for c in row])\n",
    "    return rows\n",
    "\n",
    "def load_alleles(path: str) -> Dict[str, int]:\n",
    "    uniq = []\n",
    "    for r in smart_read_table(path):\n",
    "        for c in r:\n",
    "            for token in c.replace(\",\", \" \").split():\n",
    "                if token and token not in uniq:\n",
    "                    uniq.append(token)\n",
    "    return {a: i for i, a in enumerate(sorted(uniq))}\n",
    "\n",
    "def parse_examples(path: str, allele_to_id: Dict[str, int]) -> List[Example]:\n",
    "    rows = smart_read_table(path); exs = []\n",
    "    for r in rows:\n",
    "        if len(r) < 3: continue\n",
    "        pep, score_str, allele = r[0], r[1], r[2]\n",
    "        try: score = float(score_str)\n",
    "        except Exception: continue\n",
    "        if allele not in allele_to_id:\n",
    "            allele_to_id[allele] = len(allele_to_id)\n",
    "        exs.append(Example(peptide=pep, allele=allele, score=score))\n",
    "    return exs\n",
    "\n",
    "class SeqTokenizer:\n",
    "    def __init__(self, max_len=32): self.max_len=max_len\n",
    "    def encode(self, s: str):\n",
    "        ids = [AA_TO_ID.get(ch, 0) for ch in s[:self.max_len]]\n",
    "        if len(ids) < self.max_len: ids += [0]*(self.max_len - len(ids))\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "class PeptideDataset(Dataset):\n",
    "    def __init__(self, exs, allele_to_id, pep_len=32, tcr_len=24):\n",
    "        self.exs = exs; self.allele_to_id = allele_to_id\n",
    "        self.tok_p = SeqTokenizer(pep_len); self.tok_t = SeqTokenizer(tcr_len)\n",
    "    def __len__(self): return len(self.exs)\n",
    "    def __getitem__(self, i):\n",
    "        e = self.exs[i]\n",
    "        pep = self.tok_p.encode(e.peptide)\n",
    "        tcr = self.tok_t.encode(e.tcr or \"CASSIRSSYEQYF\")\n",
    "        all_idx = self.allele_to_id.get(e.allele, 0)\n",
    "        return pep, tcr, all_idx, float(e.score)\n",
    "\n",
    "def collate_pep(batch):\n",
    "    pep, tcr, all_idx, y = zip(*batch)\n",
    "    pep = torch.stack(pep); tcr = torch.stack(tcr)\n",
    "    all_idx = torch.tensor(all_idx, dtype=torch.long)\n",
    "    y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
    "    cytok = torch.zeros((pep.size(0), len(CYTOKINES)), dtype=torch.float32)\n",
    "    cytok[:, IDX_NONE] = 1.0\n",
    "    return pep, tcr, all_idx, cytok, y\n",
    "\n",
    "# ============================================================\n",
    "# Loss & Metrics\n",
    "# ============================================================\n",
    "#def dynamic_nonconvex_loss(pred, target, epoch=0, eps=1e-6, freq=4.0, amp=0.15, basin_depth=0.1):\n",
    "def dynamic_nonconvex_loss(pred, target, epoch=0, eps=1e-6):\n",
    "    e = pred - target\n",
    "    base = torch.sqrt(torch.abs(e) + eps)\n",
    "   # ripple = amp * torch.sin(freq * e) ** 2\n",
    "   # basin = basin_depth * (e ** 4 - e ** 2)\n",
    "   # decay = math.exp(-0.02 * epoch)     # faster decay\n",
    "    #return torch.mean(base + decay * (ripple + basin))\n",
    "    return torch.mean(base)\n",
    "\n",
    "def regression_metrics(preds, targets):\n",
    "    preds = np.asarray(preds).flatten(); targets = np.asarray(targets).flatten()\n",
    "    mse = np.mean((preds - targets) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(preds - targets))\n",
    "    denom = np.sum((targets - np.mean(targets)) ** 2) + 1e-12\n",
    "    r2 = 1.0 - np.sum((targets - preds) ** 2) / denom\n",
    "    pear = pearsonr(preds, targets)[0] if len(preds) > 1 else 0.0\n",
    "    return {\"MSE\": float(mse), \"RMSE\": float(rmse), \"MAE\": float(mae), \"R2\": float(r2), \"Pearson\": float(pear)}\n",
    "\n",
    "# ============================================================\n",
    "# ImmuneNet (linear-only)\n",
    "# ============================================================\n",
    "class MiniGAT(nn.Module):\n",
    "    def __init__(self, vocab_size, dim, max_len=32, heads=4, layers=2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, dim)\n",
    "        self.pos = nn.Embedding(max_len, dim)\n",
    "        enc = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=layers)\n",
    "        self.max_len = max_len\n",
    "    def forward(self, x):\n",
    "        L = min(x.size(1), self.max_len)\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0).expand(x.size(0), L)\n",
    "        h = self.emb(x[:, :L]) + self.pos(pos)\n",
    "        h = self.encoder(h)\n",
    "        return torch.cat([h.mean(dim=1), h[:, 0, :]], dim=-1)\n",
    "\n",
    "class ImmuneNet(nn.Module):\n",
    "    def __init__(self, vocab_size, allele_count, dim=128, pep_len=32, tcr_len=24):\n",
    "        super().__init__()\n",
    "        self.pep_enc = MiniGAT(vocab_size, dim, pep_len)\n",
    "        self.tcr_enc = MiniGAT(vocab_size, dim, tcr_len)\n",
    "        self.all_emb = nn.Embedding(allele_count+1, dim)\n",
    "        hid = 256\n",
    "        in_dim = 2*dim + 2*dim + dim\n",
    "        self.backbone = nn.Sequential(nn.Linear(in_dim, hid), nn.Linear(hid, hid))\n",
    "        self.binding = nn.Linear(hid, 1)\n",
    "        self.recognition = nn.Linear(hid, 1)\n",
    "        self.cyt_fc = nn.Linear(len(CYTOKINES), 32)\n",
    "        self.response = nn.Sequential(nn.Linear(hid+32, 128), nn.Linear(128, 1))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    def encode_backbone(self, pep, tcr, all_idx):\n",
    "        pep_h = self.pep_enc(pep); tcr_h = self.tcr_enc(tcr); all_h = self.all_emb(all_idx)\n",
    "        return self.backbone(torch.cat([pep_h, tcr_h, all_h], dim=-1))\n",
    "    def forward(self, pep, tcr, all_idx, cytok):\n",
    "        z = self.encode_backbone(pep, tcr, all_idx)\n",
    "        bind = torch.sigmoid(self.binding(z))\n",
    "        recog = torch.sigmoid(self.recognition(z))\n",
    "        c = self.cyt_fc(cytok)\n",
    "        resp = self.response(torch.cat([z, c], dim=-1))\n",
    "        return bind, recog, resp\n",
    "\n",
    "# ============================================================\n",
    "# Training ImmuneNet\n",
    "# ============================================================\n",
    "def train_supervised(model, tr_dl, val_dl, device, epochs=3, lr=2e-4):\n",
    "    model.train(); opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for ep in range(1, epochs+1):\n",
    "        losses=[]\n",
    "        for pep,tcr,all_idx,cytok,y in tr_dl:\n",
    "            pep,tcr,all_idx,cytok,y=[t.to(device) for t in (pep,tcr,all_idx,cytok,y)]\n",
    "            bind,_,_=model(pep,tcr,all_idx,cytok)\n",
    "            loss=dynamic_nonconvex_loss(bind,y,epoch=ep)\n",
    "            opt.zero_grad(); loss.backward(); opt.step(); losses.append(loss.item())\n",
    "        metrics=evaluate_supervised(model,val_dl,device)\n",
    "        print(f\"[Supervised] Epoch {ep} | Loss={np.mean(losses):.6f} | MSE={metrics['MSE']:.4f} | R2={metrics['R2']:.3f} | Pearson={metrics['Pearson']:.3f}\")\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_supervised(model, dl, device):\n",
    "    model.eval(); preds=[]; targs=[]\n",
    "    for pep,tcr,all_idx,cytok,y in dl:\n",
    "        pep,tcr,all_idx,cytok=[t.to(device) for t in (pep,tcr,all_idx,cytok)]\n",
    "        _,_,resp=model(pep,tcr,all_idx,cytok)\n",
    "        preds.append(resp.cpu().numpy()); targs.append(y.numpy())\n",
    "    return regression_metrics(np.concatenate(preds), np.concatenate(targs))\n",
    "\n",
    "# ============================================================\n",
    "# Adaptive Cytokine Env\n",
    "# ============================================================\n",
    "class AdaptiveCytokineEnv(nn.Module):\n",
    "    def __init__(self, num_cyt=len(CYTOKINES), rot_eps=0.02, base_bias=0.1, action_gain=0.25):\n",
    "        super().__init__()\n",
    "        self.num_cyt=num_cyt; self.rot_eps=rot_eps; self.base_bias=base_bias; self.action_gain=action_gain\n",
    "        self.register_buffer(\"R_t\", torch.eye(num_cyt)); self.register_buffer(\"b_t\", torch.zeros(num_cyt))\n",
    "    @torch.no_grad()\n",
    "    def set_epoch(self,ep:int):\n",
    "        M=torch.randn(self.num_cyt,self.num_cyt,device=self.R_t.device)\n",
    "        Q,R=torch.linalg.qr(M); self.R_t=Q*torch.sign(torch.diag(R))\n",
    "        self.b_t=self.base_bias*torch.randn(self.num_cyt,device=self.R_t.device)\n",
    "    @torch.no_grad()\n",
    "    def _drift(self):\n",
    "        Q,R=torch.linalg.qr(self.R_t+self.rot_eps*torch.randn_like(self.R_t))\n",
    "        self.R_t=Q*torch.sign(torch.diag(R))\n",
    "        self.b_t=0.95*self.b_t+0.05*torch.randn_like(self.b_t)\n",
    "    @torch.no_grad()\n",
    "    def step(self,model,pep,tcr,all_idx,a,z,d,cytok_prev=None):\n",
    "        B,N=pep.size(0),self.num_cyt\n",
    "        cytok=torch.zeros((B,N),device=d) if cytok_prev is None else cytok_prev.clone()\n",
    "        cytok[:,IDX_NONE]=1.0\n",
    "        a_rot=a@self.R_t.to(d)\n",
    "        cytok_next=torch.clamp(cytok+self.action_gain*a_rot+self.b_t.to(d),0.0,1.0)\n",
    "        _,recog,resp=model(pep,tcr,all_idx,cytok_next)\n",
    "        reward=(0.7*resp.squeeze(-1)+0.3*recog.squeeze(-1)).detach()\n",
    "        self._drift()\n",
    "        return reward,cytok_next\n",
    "\n",
    "# ============================================================\n",
    "# PPO Policy\n",
    "# ============================================================\n",
    "class PPOPolicy(nn.Module):\n",
    "    def __init__(self,input_dim,num_actions,width=256,depth=2,mode=\"convex\",std=0.1,switch_epoch=3):\n",
    "        super().__init__()\n",
    "        self.mode=mode; self.switch_epoch=switch_epoch; self._ep=0; self._std=std\n",
    "        def mlp(out_dim):\n",
    "            layers=[]; d_in=input_dim\n",
    "            for _ in range(depth): layers+=[nn.Linear(d_in,width)]; d_in=width\n",
    "            layers.append(nn.Linear(d_in,out_dim)); return nn.ModuleList(layers)\n",
    "        self.actor_layers=mlp(num_actions); self.critic_layers=mlp(1)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight,gain=math.sqrt(2)); nn.init.zeros_(m.bias)\n",
    "    def set_epoch(self,ep:int): self._ep=ep\n",
    "    def _apply_stack(self,layers,x):\n",
    "        for layer in layers[:-1]: x=layer(x)\n",
    "        return layers[-1](x)\n",
    "    def actor_forward(self,x): return self._apply_stack(self.actor_layers,x)\n",
    "    def critic_forward(self,x): return self._apply_stack(self.critic_layers,x).squeeze(-1)\n",
    "    def act(self,x):\n",
    "        mu=self.actor_forward(x)\n",
    "        dist=torch.distributions.Normal(mu,self._std)\n",
    "        u=dist.rsample(); a=torch.tanh(u)\n",
    "        logp=dist.log_prob(u).sum(-1)-torch.log(1-a.pow(2)+1e-6).sum(-1)\n",
    "        v=self.critic_forward(x); return a,logp,v\n",
    "    def act_deterministic(self,x):\n",
    "        mu=self.actor_forward(x); v=self.critic_forward(x)\n",
    "        return torch.tanh(mu),v\n",
    "\n",
    "# ============================================================\n",
    "# PPO training & distillation\n",
    "# ============================================================\n",
    "def ppo_train_multistep(policy,make_batch,env,immune,d,episodes=3,ppo_epochs=8,multistep_steps=5,\n",
    "                        gamma=0.99,clip=0.2,max_grad_norm=0.5,lr=5e-4):\n",
    "    opt=torch.optim.Adam(policy.parameters(),lr=lr)\n",
    "    for ep in range(episodes):\n",
    "        policy.set_epoch(ep); env.set_epoch(ep)\n",
    "        S,A,OL,Gt,Adv=[],[],[],[],[]\n",
    "        while True:\n",
    "            batch=make_batch()\n",
    "            if batch is None: break\n",
    "            z=batch[\"state\"].to(d); pep,tcr,all_idx,cytok=[batch[k].to(d) for k in (\"pep\",\"tcr\",\"all\",\"cytok_init\")]\n",
    "            with torch.no_grad():\n",
    "                disc=torch.zeros(z.size(0),device=d); cy=cytok\n",
    "                for t in range(multistep_steps):\n",
    "                    a,lp,v=policy.act(z)\n",
    "                    r,cy=env.step(immune,pep,tcr,all_idx,a,z,d,cy)\n",
    "                    disc+=(gamma**t)*r\n",
    "                _,vL=policy.act_deterministic(z)\n",
    "                Aadv=(disc-vL).detach()\n",
    "            S.append(z);A.append(a);OL.append(lp);Gt.append(disc);Adv.append(Aadv)\n",
    "        if not S: continue\n",
    "        S,A,OL,Gt,Adv=map(torch.cat,(S,A,OL,Gt,Adv))\n",
    "        Adv=(Adv-Adv.mean())/(Adv.std()+1e-8)\n",
    "        for _ in range(ppo_epochs):\n",
    "            mu=policy.actor_forward(S)\n",
    "            dist=torch.distributions.Normal(mu,policy._std)\n",
    "            A_clamp=torch.clamp(A,-1+1e-6,1-1e-6)\n",
    "            u=0.5*(torch.log1p(A_clamp)-torch.log1p(-A_clamp))\n",
    "            nlp=dist.log_prob(u).sum(-1)-torch.log(1-A_clamp.pow(2)+1e-6).sum(-1)\n",
    "            ratio=torch.exp(nlp-OL)\n",
    "            aloss=-torch.min(ratio*Adv,torch.clamp(ratio,1-clip,1+clip)*Adv).mean()\n",
    "            vpred=policy.critic_forward(S)\n",
    "            closs=dynamic_nonconvex_loss(vpred,Gt,epoch=ep)\n",
    "            entropy=dist.entropy().sum(-1).mean()\n",
    "            loss=aloss+0.5*closs-0.01*entropy\n",
    "            opt.zero_grad();loss.backward()\n",
    "            if max_grad_norm>0: torch.nn.utils.clip_grad_norm_(policy.parameters(),max_grad_norm)\n",
    "            opt.step()\n",
    "        print(f\"[PPO/{policy.mode}] Ep {ep+1}/{episodes} Return={Gt.mean():.4f}\")\n",
    "    return policy\n",
    "\n",
    "def ppo_distill_loss(mu_s,mu_t,v_s,v_t,temp=1.0,alpha=0.7,std=0.1,mode=\"convex\",epoch=0):\n",
    "    var=(std*temp)**2\n",
    "    kl=((mu_s-mu_t)**2/(2*var)).mean()\n",
    "    v_mse=F.mse_loss(v_s,v_t)\n",
    "    if mode==\"convex\":\n",
    "        return alpha*kl + (1-alpha)*v_mse\n",
    "    elif mode==\"nonconvex\":\n",
    "        ripple = 0.10 * torch.sin(6.0 * (v_s - v_t))**2\n",
    "        basin  = 0.05 * ((v_s - v_t)**4 - (v_s - v_t)**2)\n",
    "        return alpha*kl + (1-alpha)*(v_mse + ripple.mean() + basin.mean())\n",
    "    elif mode==\"twostage\":\n",
    "        lam = torch.tensor(min(1.0, epoch/10.0), dtype=torch.float32, device=v_s.device)\n",
    "        ripple = 0.08 * torch.sin(6.0 * (v_s - v_t))**2\n",
    "        basin  = 0.04 * ((v_s - v_t)**4 - (v_s - v_t)**2)\n",
    "        nonconv = v_mse + ripple.mean() + basin.mean()\n",
    "        return alpha*kl + (1-alpha)*((1-lam)*v_mse + lam*nonconv)\n",
    "    else:\n",
    "        return alpha*kl + (1-alpha)*v_mse\n",
    "\n",
    "def ppo_distill(teacher,student,make_batch,d,epochs=6,lr=1e-4,temp=1.0,alpha=0.7):\n",
    "    teacher.eval(); opt=torch.optim.Adam(student.parameters(),lr=lr)\n",
    "    for ep in range(1,epochs+1):\n",
    "        student.set_epoch(ep); total=0.0; nb=0\n",
    "        while True:\n",
    "            batch=make_batch()\n",
    "            if batch is None: break\n",
    "            z=batch[\"state\"].to(d)\n",
    "            with torch.no_grad():\n",
    "                mu_t=teacher.actor_forward(z); _,v_t=teacher.act_deterministic(z)\n",
    "            mu_s=student.actor_forward(z); _,v_s=student.act_deterministic(z)\n",
    "            loss=ppo_distill_loss(mu_s,mu_t,v_s,v_t,temp=temp,alpha=alpha,std=teacher._std,mode=teacher.mode,epoch=ep)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            total+=loss.item(); nb+=1\n",
    "        print(f\"[Distill/{teacher.mode}] Ep {ep:02d}/{epochs} loss={total/max(nb,1):.6f}\")\n",
    "    return student\n",
    "\n",
    "# ============================================================\n",
    "# Dynamic Evaluation: Cytokine Control (Option 2) with full stats\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def evaluate_dynamic_cytokine(student, make_batch, env, immune_model, d,\n",
    "                              episodes=3, multistep_steps=10, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Runs rollouts and computes dynamic cytokine control metrics:\n",
    "      - Return (mean cumulative reward)\n",
    "      - ΔCyt (mean per-step absolute change in cytokines, excluding NONE)\n",
    "      - Stability (1 / variance of cytokine magnitude)\n",
    "      - Corr(Reward, ||Cyt||) over time\n",
    "      - R2 between Immune Response and Reward\n",
    "      - MSE, RMSE, MAE, R2, Pearson between cytokine magnitude and reward\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    delta_c_list = []\n",
    "    stability_list = []\n",
    "    corr_list = []\n",
    "    r2_list = []\n",
    "    mse_list, rmse_list, mae_list, r2_cyt_list, pear_list = [], [], [], [], []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        student.set_epoch(ep); env.set_epoch(ep)\n",
    "        R_t_series = []\n",
    "        C_t_series = []\n",
    "        Resp_t_series = []\n",
    "\n",
    "        while True:\n",
    "            batch = make_batch()\n",
    "            if batch is None: break\n",
    "            z = batch[\"state\"].to(d)\n",
    "            pep,tcr,all_idx,cytok = [batch[k].to(d) for k in (\"pep\",\"tcr\",\"all\",\"cytok_init\")]\n",
    "            ep_return = torch.zeros(z.size(0), device=d)\n",
    "            cy = cytok\n",
    "\n",
    "            for t in range(multistep_steps):\n",
    "                a, v = student.act_deterministic(z)\n",
    "                r, cy = env.step(immune_model, pep, tcr, all_idx, a, z, d, cy)\n",
    "                _, _, resp = immune_model(pep, tcr, all_idx, cy)\n",
    "                ep_return += (gamma**t) * r\n",
    "\n",
    "                # track per-step series\n",
    "                R_t_series.append(r.detach().cpu().numpy())\n",
    "                C_t_series.append(cy.detach().cpu().numpy())\n",
    "                Resp_t_series.append(resp.squeeze(-1).detach().cpu().numpy())\n",
    "\n",
    "            returns.append(ep_return.mean().item())\n",
    "\n",
    "        if not C_t_series:\n",
    "            continue\n",
    "\n",
    "        # Convert to arrays [T*B, ...]\n",
    "        C = np.concatenate(C_t_series, axis=0)          # (TB, num_cyt)\n",
    "        R = np.concatenate(R_t_series, axis=0).ravel()  # (TB,)\n",
    "        RESP = np.concatenate(Resp_t_series, axis=0).ravel()\n",
    "\n",
    "        # ΔCyt: mean L1 change per step (excluding NONE channel)\n",
    "        C_eff = C[:, 1:]  # exclude NONE column\n",
    "        if len(C_eff) > 1:\n",
    "            dC = np.abs(np.diff(C_eff, axis=0))\n",
    "            delta_c = float(np.mean(dC))\n",
    "            # Stability: 1 / variance of cytokine magnitude over time\n",
    "            C_mag = np.linalg.norm(C_eff, axis=1)  # Euclidean magnitude\n",
    "            stability = float(1.0 / (np.var(C_mag) + 1e-8))\n",
    "        else:\n",
    "            delta_c, stability = 0.0, 0.0\n",
    "\n",
    "        # Corr(Reward, ||C||)\n",
    "        try:\n",
    "            C_mag = np.linalg.norm(C_eff, axis=1)\n",
    "            corr = float(pearsonr(R, C_mag)[0])\n",
    "        except Exception:\n",
    "            corr = 0.0\n",
    "\n",
    "        # R2(Resp, Reward)\n",
    "        try:\n",
    "            denom = np.sum((R - np.mean(R)) ** 2) + 1e-12\n",
    "            r2 = float(1.0 - np.sum((R - RESP) ** 2) / denom)\n",
    "        except Exception:\n",
    "            r2 = 0.0\n",
    "\n",
    "        # Cytokine-vs-Reward stats (magnitude vs reward)\n",
    "        try:\n",
    "            cyt_norm = np.linalg.norm(C_eff, axis=1)\n",
    "            mse = float(np.mean((cyt_norm - R) ** 2))\n",
    "            rmse = float(np.sqrt(mse))\n",
    "            mae = float(np.mean(np.abs(cyt_norm - R)))\n",
    "            denom_c = np.sum((cyt_norm - np.mean(cyt_norm)) ** 2) + 1e-12\n",
    "            r2_cyt = float(1.0 - np.sum((cyt_norm - R) ** 2) / denom_c)\n",
    "            pear = float(pearsonr(cyt_norm, R)[0])\n",
    "        except Exception:\n",
    "            mse = rmse = mae = r2_cyt = pear = 0.0\n",
    "\n",
    "        delta_c_list.append(delta_c)\n",
    "        stability_list.append(stability)\n",
    "        corr_list.append(corr)\n",
    "        r2_list.append(r2)\n",
    "        mse_list.append(mse); rmse_list.append(rmse); mae_list.append(mae)\n",
    "        r2_cyt_list.append(r2_cyt); pear_list.append(pear)\n",
    "\n",
    "    metrics = {\n",
    "        \"Return\": float(np.mean(returns) if returns else 0.0),\n",
    "        \"DeltaCyt\": float(np.mean(delta_c_list) if delta_c_list else 0.0),\n",
    "        \"Stability\": float(np.mean(stability_list) if stability_list else 0.0),\n",
    "        \"CorrRewardCyt\": float(np.mean(corr_list) if corr_list else 0.0),\n",
    "        \"R2RespReward\": float(np.mean(r2_list) if r2_list else 0.0),\n",
    "        \"MSE\": float(np.mean(mse_list) if mse_list else 0.0),\n",
    "        \"RMSE\": float(np.mean(rmse_list) if rmse_list else 0.0),\n",
    "        \"MAE\": float(np.mean(mae_list) if mae_list else 0.0),\n",
    "        \"R2\": float(np.mean(r2_cyt_list) if r2_cyt_list else 0.0),\n",
    "        \"Pearson\": float(np.mean(pear_list) if pear_list else 0.0),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "def main():\n",
    "    import sys\n",
    "    if any(a.startswith(\"-f\") for a in sys.argv): sys.argv=[sys.argv[0]]\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train\", default=\"train_BA1.txt\")\n",
    "    parser.add_argument(\"--test\", default=\"test_BA1.txt\")\n",
    "    parser.add_argument(\"--alleles\", default=\"allelelist.txt\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=2)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    # PPO config\n",
    "    parser.add_argument(\"--ppo_multistep_steps\", type=int, default=2)\n",
    "    parser.add_argument(\"--ppo_gamma\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--ppo_clip\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--ppo_std\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--ppo_teacher_width\", type=int, default=2)\n",
    "    parser.add_argument(\"--ppo_teacher_depth\", type=int, default=2)\n",
    "    parser.add_argument(\"--ppo_teacher_episodes\", type=int, default=2)\n",
    "    parser.add_argument(\"--ppo_teacher_epochs\", type=int, default=8)\n",
    "    parser.add_argument(\"--ppo_teacher_lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--ppo_student_width\", type=int, default=2)\n",
    "    parser.add_argument(\"--ppo_student_depth\", type=int, default=2)\n",
    "    parser.add_argument(\"--ppo_distill_epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--ppo_distill_lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--ppo_distill_temp\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--ppo_distill_alpha\", type=float, default=0.7)\n",
    "    parser.add_argument(\"--modes\", type=str, default=\"convex,nonconvex,twostage\")\n",
    "    args,_ = parser.parse_known_args()\n",
    "\n",
    "    d = get_device(); seed_everything(1)\n",
    "    if not HAS_TORCH: \n",
    "        print(\"❌ PyTorch not available\"); return\n",
    "\n",
    "    # Load data\n",
    "    allele_to_id = load_alleles(args.alleles)\n",
    "    tr = parse_examples(args.train, allele_to_id)\n",
    "    ts = parse_examples(args.test, allele_to_id)\n",
    "\n",
    "    tr_ds = PeptideDataset(tr, allele_to_id); ts_ds = PeptideDataset(ts, allele_to_id)\n",
    "    N=len(tr_ds); idx=list(range(N)); random.shuffle(idx)\n",
    "    val=max(1,int(0.2*N))\n",
    "    tr_dl=DataLoader(Subset(tr_ds,idx[val:]),batch_size=args.batch_size,shuffle=True,collate_fn=collate_pep)\n",
    "    val_dl=DataLoader(Subset(tr_ds,idx[:val]),batch_size=args.batch_size,shuffle=False,collate_fn=collate_pep)\n",
    "    ts_dl=DataLoader(ts_ds,batch_size=args.batch_size,shuffle=False,collate_fn=collate_pep)\n",
    "\n",
    "    # Train ImmuneNet (linear-only) and freeze for PPO\n",
    "    immune=ImmuneNet(len(AA_VOCAB),len(allele_to_id)).to(d)\n",
    "    print(\"=== Supervised training ImmuneNet (linear-only) ===\")\n",
    "    train_supervised(immune,tr_dl,val_dl,d,epochs=args.epochs)\n",
    "    test_metrics = evaluate_supervised(immune, ts_dl, d)\n",
    "    print(\n",
    "        f\"[Test ImmuneNet] MSE={test_metrics['MSE']:.4f} | RMSE={test_metrics['RMSE']:.4f} | \"\n",
    "        f\"MAE={test_metrics['MAE']:.4f} | R2={test_metrics['R2']:.3f} | \"\n",
    "        f\"Pearson={test_metrics['Pearson']:.3f}\"\n",
    "    )\n",
    "    immune.eval()\n",
    "\n",
    "    # Batch builder for PPO\n",
    "    def make_batch_gen():\n",
    "        def make_batch():\n",
    "            if not hasattr(make_batch,\"it\"): make_batch.it=iter(tr_dl)\n",
    "            try: pep,tcr,all_idx,cytok,y=next(make_batch.it)\n",
    "            except StopIteration:\n",
    "                make_batch.it=iter(tr_dl); return None\n",
    "            with torch.no_grad():\n",
    "                z=immune.encode_backbone(pep.to(d),tcr.to(d),all_idx.to(d)).detach()\n",
    "            return {\"state\":z,\"pep\":pep,\"tcr\":tcr,\"all\":all_idx,\"cytok_init\":cytok}\n",
    "        return make_batch\n",
    "\n",
    "    # Compare modes\n",
    "    modes_list=[m.strip() for m in args.modes.split(\",\") if m.strip()]\n",
    "    results_metrics=[]  # list of (mode, metrics_dict)\n",
    "    for mode in modes_list:\n",
    "        print(f\"\\n===== Mode: {mode.upper()} =====\")\n",
    "        env=AdaptiveCytokineEnv().to(d)\n",
    "\n",
    "        # Teacher\n",
    "        teacher=PPOPolicy(256,len(CYTOKINES),\n",
    "                          width=args.ppo_teacher_width, depth=args.ppo_teacher_depth,\n",
    "                          mode=mode, std=args.ppo_std).to(d)\n",
    "        make_batch=make_batch_gen()\n",
    "        teacher=ppo_train_multistep(teacher,make_batch,env,immune,d,\n",
    "                                    episodes=args.ppo_teacher_episodes,\n",
    "                                    ppo_epochs=args.ppo_teacher_epochs,\n",
    "                                    multistep_steps=args.ppo_multistep_steps,\n",
    "                                    gamma=args.ppo_gamma, clip=args.ppo_clip,\n",
    "                                    lr=args.ppo_teacher_lr)\n",
    "\n",
    "        # Student (distillation)\n",
    "        student=PPOPolicy(256,len(CYTOKINES),\n",
    "                          width=args.ppo_student_width, depth=args.ppo_student_depth,\n",
    "                          mode=mode, std=args.ppo_std).to(d)\n",
    "        make_batch=make_batch_gen()\n",
    "        student=ppo_distill(teacher,student,make_batch,d,\n",
    "                            epochs=args.ppo_distill_epochs, lr=args.ppo_distill_lr,\n",
    "                            temp=args.ppo_distill_temp, alpha=args.ppo_distill_alpha)\n",
    "\n",
    "        # Dynamic evaluation (Option 2)\n",
    "        make_batch=make_batch_gen()\n",
    "        dyn = evaluate_dynamic_cytokine(student, make_batch, env, immune, d,\n",
    "                                        episodes=3, multistep_steps=args.ppo_multistep_steps,\n",
    "                                        gamma=args.ppo_gamma)\n",
    "\n",
    "        print(\n",
    "          f\"[Dynamic/{mode}] Return={dyn['Return']:.4f} | \"\n",
    "          f\"ΔCyt={dyn['DeltaCyt']:.4f} | Stability={dyn['Stability']:.3f} | \"\n",
    "          f\"Corr(Reward,||C||)={dyn['CorrRewardCyt']:.3f} | R2(Resp,Reward)={dyn['R2RespReward']:.3f} | \"\n",
    "          f\"MSE={dyn['MSE']:.4f} | RMSE={dyn['RMSE']:.4f} | MAE={dyn['MAE']:.4f} | \"\n",
    "          f\"R2={dyn['R2']:.3f} | Pearson={dyn['Pearson']:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Save student for reuse\n",
    "        torch.save(student.state_dict(),f\"ppo_student_{mode}.pt\")\n",
    "\n",
    "        results_metrics.append((mode, dyn))\n",
    "\n",
    "    # Write comparison CSV\n",
    "    out_csv = \"ppo_dynamic_cytokine_comparison.csv\"\n",
    "    with open(out_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\n",
    "            \"Mode\",\"Return\",\"DeltaCyt\",\"Stability\",\n",
    "            \"CorrRewardCyt\",\"R2RespReward\",\n",
    "            \"MSE\",\"RMSE\",\"MAE\",\"R2\",\"Pearson\"\n",
    "        ])\n",
    "        for mode, dyn in results_metrics:\n",
    "            w.writerow([\n",
    "                mode, dyn[\"Return\"], dyn[\"DeltaCyt\"], dyn[\"Stability\"],\n",
    "                dyn[\"CorrRewardCyt\"], dyn[\"R2RespReward\"],\n",
    "                dyn[\"MSE\"], dyn[\"RMSE\"], dyn[\"MAE\"], dyn[\"R2\"], dyn[\"Pearson\"]\n",
    "            ])\n",
    "    print(f\"✅ Saved {out_csv}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    try: \n",
    "        main()\n",
    "    except SystemExit: \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5a3a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ba356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a676c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
