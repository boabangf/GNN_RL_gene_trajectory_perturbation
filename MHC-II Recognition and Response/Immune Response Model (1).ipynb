{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b138818",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training ImmuneNet (fixed convex) ===\n",
      "[Epoch 1] Val MSE=0.0745, R2=-0.1134\n",
      "[Epoch 2] Val MSE=0.0640, R2=0.0430\n",
      "[Epoch 3] Val MSE=0.0638, R2=0.0462\n",
      "[Epoch 4] Val MSE=0.0622, R2=0.0707\n",
      "[Epoch 5] Val MSE=0.4577, R2=-5.8401\n",
      "[Epoch 6] Val MSE=0.4577, R2=-5.8401\n",
      "[Epoch 7] Val MSE=0.4577, R2=-5.8401\n",
      "[Epoch 8] Val MSE=0.4577, R2=-5.8401\n",
      "[Before PPO] MSE=0.4382, RMSE=0.6619, MAE=0.6062, R2=-5.2021, Pearson=0.0000\n",
      "\n",
      "=== PPO Activation: CONVEX ===\n",
      "[PPO] Episode 1/20 — Return=-0.1873\n",
      "[PPO] Episode 2/20 — Return=-0.1861\n",
      "[PPO] Episode 3/20 — Return=-0.1862\n",
      "[PPO] Episode 4/20 — Return=-0.1861\n",
      "[PPO] Episode 5/20 — Return=-0.1862\n",
      "[PPO] Episode 6/20 — Return=-0.1861\n",
      "[PPO] Episode 7/20 — Return=-0.1864\n",
      "[PPO] Episode 8/20 — Return=-0.1862\n",
      "[PPO] Episode 9/20 — Return=-0.1864\n",
      "[PPO] Episode 10/20 — Return=-0.1863\n",
      "[PPO] Episode 11/20 — Return=-0.1862\n",
      "[PPO] Episode 12/20 — Return=-0.1860\n",
      "[PPO] Episode 13/20 — Return=-0.1863\n",
      "[PPO] Episode 14/20 — Return=-0.1862\n",
      "[PPO] Episode 15/20 — Return=-0.1863\n",
      "[PPO] Episode 16/20 — Return=-0.1863\n",
      "[PPO] Episode 17/20 — Return=-0.1863\n",
      "[PPO] Episode 18/20 — Return=-0.1862\n",
      "[PPO] Episode 19/20 — Return=-0.1863\n",
      "[PPO] Episode 20/20 — Return=-0.1864\n",
      "[After PPO] MSE=0.4382, RMSE=0.6619, MAE=0.6062, R2=-5.2021, Pearson=0.0000\n",
      "\n",
      "=== PPO Activation: NONCONVEX ===\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Immune RL with PPO + Gated ReLU (Fixed ImmuneNet Initialization + PPO-Specific Activation)\n",
    "------------------------------------------------------------------------------------------\n",
    "- ImmuneNet is trained once with a fixed activation (convex or nonconvex).\n",
    "- Its trained weights are reused identically across all PPO runs.\n",
    "- PPO actor/critic activation varies per run: convex, nonconvex, or twostage.\n",
    "- Nonconvex loss is used for ImmuneNet and PPO critic.\n",
    "- Results saved to gated_relu_comparison.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import csv, argparse, random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from scipy.stats import pearsonr\n",
    "from copy import deepcopy\n",
    "\n",
    "# =========================\n",
    "# Optional PyTorch\n",
    "# =========================\n",
    "HAS_TORCH = True\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader, Subset\n",
    "except Exception:\n",
    "    HAS_TORCH = False\n",
    "\n",
    "# =========================\n",
    "# Constants & utils\n",
    "# =========================\n",
    "AA_VOCAB = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "AA_TO_ID = {a: i + 1 for i, a in enumerate(AA_VOCAB)}\n",
    "CYTOKINES = [\"NONE\", \"IL2\", \"IFNG\", \"IL10\", \"TNFA\"]\n",
    "CYTOKINE_TO_ID = {c: i for i, c in enumerate(CYTOKINES)}\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    if HAS_TORCH:\n",
    "        torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "@dataclass\n",
    "class Example:\n",
    "    peptide: str\n",
    "    allele: str\n",
    "    score: float\n",
    "    motif: Optional[str] = None\n",
    "    tcr: Optional[str] = None\n",
    "\n",
    "# =========================\n",
    "# I/O + Tokenizer\n",
    "# =========================\n",
    "def smart_read_table(path: str) -> List[List[str]]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", newline=\"\") as f:\n",
    "        sample = f.read(2048); f.seek(0)\n",
    "        import csv as _csv\n",
    "        try:\n",
    "            dialect = _csv.Sniffer().sniff(sample, delimiters=\"\\t,;\")\n",
    "        except Exception:\n",
    "            class dialect: delimiter = \",\"\n",
    "        reader = _csv.reader(f, dialect)\n",
    "        for row in reader:\n",
    "            if row: rows.append([c.strip() for c in row])\n",
    "    return rows\n",
    "\n",
    "def load_alleles(path: str) -> Dict[str, int]:\n",
    "    uniq = []\n",
    "    for r in smart_read_table(path):\n",
    "        for c in r:\n",
    "            for token in c.replace(\",\", \" \").split():\n",
    "                if token and token not in uniq:\n",
    "                    uniq.append(token)\n",
    "    return {a: i for i, a in enumerate(sorted(uniq))}\n",
    "\n",
    "def parse_examples(path: str, allele_to_id: Dict[str, int]) -> List[Example]:\n",
    "    rows = smart_read_table(path); exs = []\n",
    "    for r in rows:\n",
    "        if len(r) < 3: continue\n",
    "        pep, score_str, allele = r[0], r[1], r[2]\n",
    "        try:\n",
    "            score = float(score_str)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if allele not in allele_to_id:\n",
    "            allele_to_id[allele] = len(allele_to_id)\n",
    "        exs.append(Example(peptide=pep, allele=allele, score=score))\n",
    "    return exs\n",
    "\n",
    "class SeqTokenizer:\n",
    "    def __init__(self, max_len=32): self.max_len = max_len\n",
    "    def encode_ids(self, s: str):\n",
    "        ids = [AA_TO_ID.get(ch, 0) for ch in s[:self.max_len]]\n",
    "        if len(ids) < self.max_len: ids += [0]*(self.max_len - len(ids))\n",
    "        return ids\n",
    "    def encode(self, s: str):\n",
    "        arr = np.asarray(self.encode_ids(s), dtype=np.int64)\n",
    "        return torch.tensor(arr, dtype=torch.long) if HAS_TORCH else arr\n",
    "\n",
    "# =========================\n",
    "# Loss + Metrics\n",
    "# =========================\n",
    "def nonconvex_loss(pred, target, eps: float = 1e-6):\n",
    "    e = pred - target\n",
    "    return torch.mean(torch.sqrt(torch.abs(e) + eps))\n",
    "\n",
    "def regression_metrics(preds: np.ndarray, targets: np.ndarray) -> Dict[str, float]:\n",
    "    preds = np.asarray(preds).flatten()\n",
    "    targets = np.asarray(targets).flatten()\n",
    "    mse = float(np.mean((preds - targets) ** 2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae = float(np.mean(np.abs(preds - targets)))\n",
    "    denom = float(np.sum((targets - np.mean(targets)) ** 2) + 1e-12)\n",
    "    r2 = float(1.0 - np.sum((targets - preds) ** 2) / denom)\n",
    "    pear = float(pearsonr(preds, targets)[0]) if len(preds) > 1 and np.std(preds) > 0 and np.std(targets) > 0 else 0.0\n",
    "    return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2, \"Pearson\": pear}\n",
    "\n",
    "# =========================\n",
    "# Gated ReLU + Models\n",
    "# =========================\n",
    "if HAS_TORCH:\n",
    "    class GatedReLU(nn.Module):\n",
    "        def __init__(self, dim: int, mode=\"convex\", switch_epoch=5):\n",
    "            super().__init__()\n",
    "            self.mode = mode\n",
    "            self.switch_epoch = switch_epoch\n",
    "            self.switched = False\n",
    "            self.a = nn.Parameter(torch.ones(dim))\n",
    "            self.b = nn.Parameter(torch.zeros(dim))\n",
    "            self.register_buffer(\"_epoch\", torch.zeros(1, dtype=torch.long), persistent=False)\n",
    "        def set_epoch(self, epoch: int): self._epoch[0] = epoch\n",
    "        def trigger_switch(self):\n",
    "            if not self.switched:\n",
    "                print(f\"⚡ GatedReLU switched to nonconvex at episode {self._epoch.item()}\")\n",
    "                self.switched = True\n",
    "        def _convex(self, x): return torch.max(self.a * x + self.b, x)\n",
    "        def _nonconvex(self, x): return torch.min(self.a * x + self.b, x)\n",
    "        def forward(self, x):\n",
    "            if self.mode == \"convex\": return self._convex(x)\n",
    "            elif self.mode == \"nonconvex\": return self._nonconvex(x)\n",
    "            elif self.mode == \"twostage\":\n",
    "                if self.switched or (self._epoch.item() >= self.switch_epoch):\n",
    "                    return self._nonconvex(x)\n",
    "                else:\n",
    "                    return self._convex(x)\n",
    "            else: raise ValueError(f\"Unknown mode: {self.mode}\")\n",
    "\n",
    "    class MiniGAT(nn.Module):\n",
    "        def __init__(self, vocab_size, dim, max_len=32, heads=4, layers=2):\n",
    "            super().__init__()\n",
    "            self.emb = nn.Embedding(vocab_size+1, dim)\n",
    "            self.pos = nn.Embedding(max_len, dim)\n",
    "            enc = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, batch_first=True)\n",
    "            self.encoder = nn.TransformerEncoder(enc, num_layers=layers)\n",
    "            self.max_len = max_len\n",
    "        def forward(self, x):\n",
    "            L = min(x.size(1), self.max_len)\n",
    "            pos = torch.arange(L, device=x.device).unsqueeze(0).expand(x.size(0), L)\n",
    "            h = self.emb(x[:, :L]) + self.pos(pos)\n",
    "            h = self.encoder(h)\n",
    "            mean, cls = h.mean(dim=1), h[:, 0, :]\n",
    "            return torch.cat([mean, cls], dim=-1)\n",
    "\n",
    "    class ImmuneNet(nn.Module):\n",
    "        def __init__(self, vocab_size, allele_count, dim=128, pep_len=32, tcr_len=24,\n",
    "                     act_mode=\"convex\"):\n",
    "            super().__init__()\n",
    "            self.pep_enc = MiniGAT(vocab_size, dim, pep_len)\n",
    "            self.tcr_enc = MiniGAT(vocab_size, dim, tcr_len)\n",
    "            self.all_emb = nn.Embedding(allele_count+1, dim)\n",
    "            in_dim = 2*dim + 2*dim + dim\n",
    "            hid = 256\n",
    "            def act(): return GatedReLU(hid, act_mode)\n",
    "            self.backbone = nn.Sequential(nn.Linear(in_dim, hid), act(),\n",
    "                                          nn.Linear(hid, hid), act())\n",
    "            self.binding = nn.Linear(hid, 1)\n",
    "            self.recognition = nn.Linear(hid, 1)\n",
    "            self.cyt_fc = nn.Linear(len(CYTOKINES), 32)\n",
    "            self.response = nn.Sequential(nn.Linear(hid+32, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "        def encode_backbone(self, pep, tcr, allele):\n",
    "            pep_h = self.pep_enc(pep)\n",
    "            tcr_h = self.tcr_enc(tcr)\n",
    "            all_h = self.all_emb(allele)\n",
    "            return self.backbone(torch.cat([pep_h, tcr_h, all_h], dim=-1))\n",
    "        def forward(self, pep, tcr, allele, cytok_onehot):\n",
    "            z = self.encode_backbone(pep, tcr, allele)\n",
    "            bind = torch.sigmoid(self.binding(z))\n",
    "            recog = torch.sigmoid(self.recognition(z))\n",
    "            c = F.relu(self.cyt_fc(cytok_onehot))\n",
    "            resp = self.response(torch.cat([z, c], dim=-1))\n",
    "            return bind, recog, resp\n",
    "\n",
    "# =========================\n",
    "# PPO\n",
    "# =========================\n",
    "if HAS_TORCH:\n",
    "    class PPOPolicy(nn.Module):\n",
    "        def __init__(self, input_dim, num_actions, hidden=256, act_mode=\"convex\", switch_epoch=5):\n",
    "            super().__init__()\n",
    "            def act(): return GatedReLU(hidden, act_mode, switch_epoch)\n",
    "            self.actor = nn.Sequential(nn.Linear(input_dim, hidden), act(), nn.Linear(hidden, num_actions))\n",
    "            self.critic = nn.Sequential(nn.Linear(input_dim, hidden), act(), nn.Linear(hidden, 1))\n",
    "            self.act_mode = act_mode; self.switch_epoch = switch_epoch\n",
    "        def set_epoch(self, epoch: int):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, GatedReLU):\n",
    "                    m.set_epoch(epoch)\n",
    "                    if self.act_mode == \"twostage\" and not m.switched and epoch >= self.switch_epoch:\n",
    "                        m.trigger_switch()\n",
    "        def act(self, x):\n",
    "            logits = self.actor(x)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            a = dist.sample(); logp = dist.log_prob(a)\n",
    "            v = self.critic(x).squeeze(-1)\n",
    "            return a, logp, v\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_state(model, pep, tcr, allele):\n",
    "        return model.encode_backbone(pep, tcr, allele).detach()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def env_step(model, pep, tcr, allele, act, device):\n",
    "        B = pep.size(0)\n",
    "        cytok = torch.zeros((B, len(CYTOKINES)), device=device)\n",
    "        cytok[torch.arange(B), act] = 1.0\n",
    "        _, recog, resp = model(pep, tcr, allele, cytok)\n",
    "        return (0.7*resp + 0.3*recog).squeeze(-1).detach()\n",
    "\n",
    "    def ppo_train(model, loader, device, ppo_epochs=3, episodes=5, clip=0.2, act_mode=\"convex\", switch_epoch=5):\n",
    "        policy = PPOPolicy(256, len(CYTOKINES), hidden=256, act_mode=act_mode, switch_epoch=switch_epoch).to(device)\n",
    "        opt = torch.optim.Adam(policy.parameters(), lr=5e-4)\n",
    "        for ep in range(episodes):\n",
    "            policy.set_epoch(ep)\n",
    "            states, actions, old_logp, returns, adv = [], [], [], [], []\n",
    "            for pep, tcr, all_idx, cytok, y in loader:\n",
    "                pep, tcr, all_idx = pep.to(device), tcr.to(device), all_idx.to(device)\n",
    "                with torch.no_grad():\n",
    "                    z = extract_state(model, pep, tcr, all_idx)\n",
    "                    a, logp, v = policy.act(z)\n",
    "                    G = env_step(model, pep, tcr, all_idx, a, device)\n",
    "                    A = (G - v).detach()\n",
    "                states.append(z); actions.append(a); old_logp.append(logp); returns.append(G); adv.append(A)\n",
    "            states, actions = torch.cat(states), torch.cat(actions)\n",
    "            old_logp, returns, adv = torch.cat(old_logp), torch.cat(returns), torch.cat(adv)\n",
    "            adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "            for _ in range(ppo_epochs):\n",
    "                logits = policy.actor(states)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                new_logp = dist.log_prob(actions)\n",
    "                ratio = torch.exp(new_logp - old_logp)\n",
    "                surr1 = ratio * adv\n",
    "                surr2 = torch.clamp(ratio, 1.0 - clip, 1.0 + clip) * adv\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_loss = nonconvex_loss(policy.critic(states).squeeze(-1), returns)\n",
    "                loss = actor_loss + 0.5*critic_loss\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "            print(f\"[PPO] Episode {ep+1}/{episodes} — Return={returns.mean().item():.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Dataset + Training + Eval\n",
    "# =========================\n",
    "if HAS_TORCH:\n",
    "    class PeptideDataset(Dataset):\n",
    "        def __init__(self, examples, allele_to_id, pep_len=32, tcr_len=24):\n",
    "            self.examples = examples; self.allele_to_id = allele_to_id\n",
    "            self.tok_p = SeqTokenizer(pep_len); self.tok_t = SeqTokenizer(tcr_len)\n",
    "        def __len__(self): return len(self.examples)\n",
    "        def __getitem__(self, idx):\n",
    "            ex = self.examples[idx]\n",
    "            pep = self.tok_p.encode(ex.peptide)\n",
    "            tcr = self.tok_t.encode(ex.tcr or \"CASSIRSSYEQYF\")\n",
    "            all_idx = self.allele_to_id.get(ex.allele, 0)\n",
    "            return pep, tcr, all_idx, float(ex.score)\n",
    "\n",
    "    def collate(batch):\n",
    "        pep, tcr, all_idx, y = zip(*batch)\n",
    "        pep = torch.stack(pep); tcr = torch.stack(tcr)\n",
    "        all_idx = torch.tensor(all_idx, dtype=torch.long)\n",
    "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
    "        cytok = torch.zeros((pep.size(0), len(CYTOKINES)), dtype=torch.float32)\n",
    "        cytok[:, CYTOKINE_TO_ID[\"NONE\"]] = 1.0\n",
    "        return pep, tcr, all_idx, cytok, y\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_loader_metrics(model, loader, device):\n",
    "        model.eval(); preds, targets = [], []\n",
    "        for pep, tcr, all_idx, cytok, y in loader:\n",
    "            pep, tcr, all_idx, cytok = pep.to(device), tcr.to(device), all_idx.to(device), cytok.to(device)\n",
    "            bind, _, _ = model(pep, tcr, all_idx, cytok)\n",
    "            preds.extend(bind.cpu().numpy()); targets.extend(y.numpy())\n",
    "        return regression_metrics(np.array(preds), np.array(targets))\n",
    "\n",
    "    def train_supervised(model, train_loader, val_loader, device, epochs=10):\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        for ep in range(epochs):\n",
    "            model.train()\n",
    "            for pep, tcr, all_idx, cytok, y in train_loader:\n",
    "                pep, tcr, all_idx, cytok, y = pep.to(device), tcr.to(device), all_idx.to(device), cytok.to(device), y.to(device)\n",
    "                bind, _, _ = model(pep, tcr, all_idx, cytok)\n",
    "                loss = nonconvex_loss(bind, y)\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "            vm = eval_loader_metrics(model, val_loader, device)\n",
    "            print(f\"[Epoch {ep+1}] Val MSE={vm['MSE']:.4f}, R2={vm['R2']:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    import sys\n",
    "    if any(a.startswith(\"-f\") or \"kernel\" in a for a in sys.argv):\n",
    "        sys.argv = [sys.argv[0]]\n",
    "\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('--train', default='train_BA1.txt')\n",
    "    p.add_argument('--test', default='test_BA1.txt')\n",
    "    p.add_argument('--alleles', default='allelelist.txt')\n",
    "    p.add_argument('--epochs', type=int, default=8)\n",
    "    p.add_argument('--batch_size', type=int, default=64)\n",
    "    p.add_argument('--ppo_episodes', type=int, default=20)\n",
    "    p.add_argument('--ppo_epochs', type=int, default=3)\n",
    "    p.add_argument('--pep_len', type=int, default=32)\n",
    "    p.add_argument('--tcr_len', type=int, default=24)\n",
    "    p.add_argument('--immune_act_mode', type=str, default='convex', choices=['convex','nonconvex'])\n",
    "    p.add_argument('--ppo_switch_epoch', type=int, default=4)\n",
    "    p.add_argument('--val_ratio', type=float, default=0.2)\n",
    "    a, _ = p.parse_known_args()\n",
    "\n",
    "    seed_everything(1)\n",
    "    if not HAS_TORCH:\n",
    "        print(\"⚠️ PyTorch not available.\"); return\n",
    "\n",
    "    d = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    allele_to_id = load_alleles(a.alleles)\n",
    "    tr = parse_examples(a.train, allele_to_id)\n",
    "    ts = parse_examples(a.test, allele_to_id)\n",
    "\n",
    "    full_train_ds = PeptideDataset(tr, allele_to_id, a.pep_len, a.tcr_len)\n",
    "    N = len(full_train_ds)\n",
    "    idx = list(range(N)); random.shuffle(idx)\n",
    "    v = int(max(1, round(a.val_ratio * N)))\n",
    "    tr_ds = Subset(full_train_ds, idx[v:])\n",
    "    va_ds = Subset(full_train_ds, idx[:v])\n",
    "    te_ds = PeptideDataset(ts, allele_to_id, a.pep_len, a.tcr_len)\n",
    "\n",
    "    trdl = DataLoader(tr_ds, batch_size=a.batch_size, shuffle=True, collate_fn=collate)\n",
    "    vadl = DataLoader(va_ds, batch_size=a.batch_size, shuffle=False, collate_fn=collate)\n",
    "    tsdl = DataLoader(te_ds, batch_size=a.batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "    # 1️⃣ Train ImmuneNet once\n",
    "    print(f\"\\n=== Training ImmuneNet (fixed {a.immune_act_mode}) ===\")\n",
    "    base_model = ImmuneNet(len(AA_VOCAB), len(allele_to_id),\n",
    "                           act_mode=a.immune_act_mode,\n",
    "                           pep_len=a.pep_len, tcr_len=a.tcr_len).to(d)\n",
    "    train_supervised(base_model, trdl, vadl, d, epochs=a.epochs)\n",
    "    base_state = deepcopy(base_model.state_dict())  # fixed initialization snapshot\n",
    "\n",
    "    before = eval_loader_metrics(base_model, tsdl, d)\n",
    "    print(\"[Before PPO] \" + \", \".join([f\"{k}={v:.4f}\" for k, v in before.items()]))\n",
    "\n",
    "    # 2️⃣ PPO runs with different activation modes\n",
    "    ppo_modes = [\"convex\", \"nonconvex\", \"twostage\"]\n",
    "    results = []\n",
    "\n",
    "    for ppo_mode in ppo_modes:\n",
    "        print(f\"\\n=== PPO Activation: {ppo_mode.upper()} ===\")\n",
    "        model = ImmuneNet(len(AA_VOCAB), len(allele_to_id),\n",
    "                          act_mode=a.immune_act_mode,\n",
    "                          pep_len=a.pep_len, tcr_len=a.tcr_len).to(d)\n",
    "        model.load_state_dict(base_state)  # identical weights\n",
    "\n",
    "        ppo_train(model, trdl, d,\n",
    "                  ppo_epochs=a.ppo_epochs,\n",
    "                  episodes=a.ppo_episodes,\n",
    "                  act_mode=ppo_mode,\n",
    "                  switch_epoch=a.ppo_switch_epoch)\n",
    "\n",
    "        after = eval_loader_metrics(model, tsdl, d)\n",
    "        print(\"[After PPO] \" + \", \".join([f\"{k}={v:.4f}\" for k, v in after.items()]))\n",
    "        results.append([f\"Immune({a.immune_act_mode}) + PPO({ppo_mode})\",\n",
    "                        after[\"MSE\"], after[\"RMSE\"], after[\"MAE\"], after[\"R2\"], after[\"Pearson\"]])\n",
    "\n",
    "    with open(\"gated_relu_comparison.csv\", \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Setup\", \"MSE\", \"RMSE\", \"MAE\", \"R2\", \"Pearson\"])\n",
    "        writer.writerows(results)\n",
    "    print(\"\\n✅ Results saved to gated_relu_comparison.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except SystemExit:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b0dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
