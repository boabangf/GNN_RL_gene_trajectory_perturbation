{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b138818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Supervised] Epoch 1/12 loss=0.0474 val_mse=0.0451\n",
      "[Supervised] Epoch 2/12 loss=0.0425 val_mse=0.0444\n",
      "[Supervised] Epoch 3/12 loss=0.0410 val_mse=0.0453\n",
      "[Supervised] Epoch 4/12 loss=0.0401 val_mse=0.0440\n",
      "[Supervised] Epoch 5/12 loss=0.0394 val_mse=0.0453\n",
      "[Supervised] Epoch 6/12 loss=0.0389 val_mse=0.0461\n",
      "[Supervised] Epoch 7/12 loss=0.0383 val_mse=0.0448\n",
      "[Supervised] Epoch 8/12 loss=0.0379 val_mse=0.0446\n",
      "[Supervised] Epoch 9/12 loss=0.0376 val_mse=0.0457\n",
      "[Supervised] Epoch 10/12 loss=0.0373 val_mse=0.0445\n",
      "[Supervised] Epoch 11/12 loss=0.0369 val_mse=0.0464\n",
      "[Supervised] Epoch 12/12 loss=0.0366 val_mse=0.0450\n",
      "\n",
      "== Supervised Test Metrics ==\n",
      "MSE: 0.045001\n",
      "RMSE: 0.212134\n",
      "R2: 0.363033\n",
      "Pearson: 0.610868\n",
      "[PPO] epoch 1/6  actor_loss=-0.1475  critic_loss=0.0086\n",
      "[PPO] epoch 2/6  actor_loss=-0.1315  critic_loss=0.0111\n",
      "[PPO] epoch 3/6  actor_loss=-0.1272  critic_loss=0.0099\n",
      "[PPO] epoch 4/6  actor_loss=-0.1246  critic_loss=0.0097\n",
      "[PPO] epoch 5/6  actor_loss=-0.1258  critic_loss=0.0079\n",
      "[PPO] epoch 6/6  actor_loss=-0.1259  critic_loss=0.0089\n",
      "\n",
      "== PPO Mutation Demonstration ==\n",
      "Start: LCVTQVLMMRTTWAL | Best mutant: LCVTQVLMMRTTWAL | Pred cytokine: 0.3724\n",
      "Start: DRASYRAHWQDDDVT | Best mutant: DRASYRAIWQDDDVT | Pred cytokine: 0.3132\n",
      "Start: KNWMTETLLVQNANPDCKTI | Best mutant: KNWMTETILVQNANPDCKTI | Pred cytokine: 0.3161\n",
      "Start: PELQNFLNFLEANGL | Best mutant: PELQNFLIFLEANGL | Pred cytokine: 0.4796\n",
      "Start: NQFGSVPAVTISCMT | Best mutant: NQFGSVPIVTISCMT | Pred cytokine: 0.3941\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# -------------------------------\n",
    "# Config & Utils\n",
    "# -------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AMINO_ACIDS = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "AA_TO_IDX = {aa: i + 1 for i, aa in enumerate(AMINO_ACIDS)}  # 0 for pad\n",
    "IDX_TO_AA = {v: k for k, v in AA_TO_IDX.items()}\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def encode_seq(seq: str, max_len: int) -> List[int]:\n",
    "    seq = (seq or \"\")[:max_len]\n",
    "    enc = [AA_TO_IDX.get(ch, 0) for ch in seq]\n",
    "    if len(enc) < max_len:\n",
    "        enc += [0] * (max_len - len(enc))\n",
    "    return enc\n",
    "\n",
    "def one_hot_indices(idx_seq: List[int], vocab_size: int=21) -> np.ndarray:\n",
    "    # idx_seq values in [0..vocab_size-1]; 0=padding -> one-hot zeros\n",
    "    L = len(idx_seq)\n",
    "    out = np.zeros((L, vocab_size), dtype=np.float32)\n",
    "    for i, idx in enumerate(idx_seq):\n",
    "        if idx > 0:\n",
    "            out[i, idx] = 1.0\n",
    "    return out\n",
    "\n",
    "# -------------------------------\n",
    "# Data Loading\n",
    "# -------------------------------\n",
    "def load_dataset(train_path: str, test_path: str):\n",
    "    # Expect 4 columns: peptide, binding_aff, mhc_seq, mutant\n",
    "    train_df = pd.read_csv(train_path, sep=\"\\t\", header=None,\n",
    "                           names=[\"peptide\", \"binding_aff\", \"mhc_seq\", \"mutant\"])\n",
    "    test_df  = pd.read_csv(test_path, sep=\"\\t\", header=None,\n",
    "                           names=[\"peptide\", \"binding_aff\", \"mhc_seq\", \"mutant\"])\n",
    "\n",
    "    # Normalize binding to 0..1 for 'cytokine' proxy\n",
    "    def normalize_col(col):\n",
    "        mn, mx = col.min(), col.max()\n",
    "        if mx - mn < 1e-12:\n",
    "            return np.zeros_like(col, dtype=np.float32)\n",
    "        return ((col - mn) / (mx - mn)).astype(np.float32)\n",
    "\n",
    "    train_df[\"cytokine\"] = normalize_col(train_df[\"binding_aff\"].values)\n",
    "    test_df[\"cytokine\"]  = normalize_col(test_df[\"binding_aff\"].values)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "class PMHCDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, max_pep_len: int = 15, max_mhc_len: int = 10):\n",
    "        self.max_pep = max_pep_len\n",
    "        self.max_mhc = max_mhc_len\n",
    "        self.peptides = [encode_seq(p, self.max_pep) for p in df[\"peptide\"].astype(str).tolist()]\n",
    "        self.mhcs     = [encode_seq(m, self.max_mhc) for m in df[\"mhc_seq\"].astype(str).tolist()]\n",
    "        self.y        = df[\"cytokine\"].astype(np.float32).values.reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pep = torch.tensor(self.peptides[idx], dtype=torch.long)\n",
    "        mhc = torch.tensor(self.mhcs[idx], dtype=torch.long)\n",
    "        y   = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        return pep, mhc, y\n",
    "\n",
    "# -------------------------------\n",
    "# Supervised Model\n",
    "# -------------------------------\n",
    "class ImmuneResponseModel(nn.Module):\n",
    "    def __init__(self, vocab_size=21, emb_dim=64, hidden=128):\n",
    "        super().__init__()\n",
    "        self.pep_emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.mhc_emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.conv_p  = nn.Conv1d(emb_dim, hidden, kernel_size=3, padding=1)\n",
    "        self.conv_m  = nn.Conv1d(emb_dim, hidden, kernel_size=3, padding=1)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden*2, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, pep, mhc):\n",
    "        p = self.pep_emb(pep).permute(0,2,1)     # [B, D, L]\n",
    "        m = self.mhc_emb(mhc).permute(0,2,1)     # [B, D, L]\n",
    "        p = torch.max(self.conv_p(p), dim=2)[0]  # [B, H]\n",
    "        m = torch.max(self.conv_m(m), dim=2)[0]  # [B, H]\n",
    "        z = torch.cat([p, m], dim=1)\n",
    "        return self.ff(z)                        # [B,1]\n",
    "\n",
    "def train_supervised(model, train_loader, val_loader=None, epochs=10, lr=1e-3):\n",
    "    model.to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    mse = nn.MSELoss()\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for pep, mhc, y in train_loader:\n",
    "            pep, mhc, y = pep.to(DEVICE), mhc.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            pred = model(pep, mhc)\n",
    "            loss = mse(pred, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        msg = f\"[Supervised] Epoch {ep+1}/{epochs} loss={total/len(train_loader):.4f}\"\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                vy_true, vy_pred = [], []\n",
    "                for pep, mhc, y in val_loader:\n",
    "                    pred = model(pep.to(DEVICE), mhc.to(DEVICE)).cpu().numpy()\n",
    "                    vy_pred.extend(pred.reshape(-1))\n",
    "                    vy_true.extend(y.numpy().reshape(-1))\n",
    "            v_mse = mean_squared_error(vy_true, vy_pred)\n",
    "            msg += f\" val_mse={v_mse:.4f}\"\n",
    "        print(msg)\n",
    "\n",
    "# -------------------------------\n",
    "# Multi-step Mutation Environment\n",
    "# -------------------------------\n",
    "class MutationEnv:\n",
    "    \"\"\"\n",
    "    State: one-hot peptide (L_p x 21) + one-hot mhc (L_m x 21) flattened.\n",
    "    Action: choose (position, amino-acid). Discrete space of size L_p * 20.\n",
    "    Reward: delta predicted cytokine (new - old) to drive improvements.\n",
    "    Episode length: max_steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: ImmuneResponseModel, mhc_seq: str,\n",
    "                 max_pep_len: int=15, max_mhc_len: int=10, max_steps: int=8):\n",
    "        self.model = model.to(DEVICE).eval()\n",
    "        self.mhc_seq = mhc_seq\n",
    "        self.max_pep = max_pep_len\n",
    "        self.max_mhc = max_mhc_len\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        self.action_space_n = self.max_pep * 20  # 20 amino acids\n",
    "        self.obs_dim = (self.max_pep + self.max_mhc) * 21\n",
    "\n",
    "        # buffers\n",
    "        self._pep = \"\"\n",
    "        self._mhc_idx = encode_seq(self.mhc_seq, self.max_mhc)\n",
    "        self._mhc_oh  = one_hot_indices(self._mhc_idx)  # [L_m, 21]\n",
    "        self._steps = 0\n",
    "        self._current_score = 0.0\n",
    "\n",
    "    def _predict(self, peptide: str) -> float:\n",
    "        pep_t = torch.tensor([encode_seq(peptide, self.max_pep)], dtype=torch.long, device=DEVICE)\n",
    "        mhc_t = torch.tensor([self._mhc_idx], dtype=torch.long, device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            y = self.model(pep_t, mhc_t).item()\n",
    "        return float(y)\n",
    "\n",
    "    def _obs(self, peptide: str) -> torch.Tensor:\n",
    "        pep_idx = encode_seq(peptide, self.max_pep)\n",
    "        pep_oh  = one_hot_indices(pep_idx)  # [L_p, 21]\n",
    "        cat = np.concatenate([pep_oh, self._mhc_oh], axis=0).reshape(-1).astype(np.float32)\n",
    "        return torch.tensor(cat, dtype=torch.float32, device=DEVICE).unsqueeze(0)  # [1, obs_dim]\n",
    "\n",
    "    def reset(self, peptide: str):\n",
    "        self._pep = peptide\n",
    "        self._steps = 0\n",
    "        self._current_score = self._predict(peptide)\n",
    "        return self._obs(peptide)\n",
    "\n",
    "    def step(self, action: int):\n",
    "        pos = action // 20\n",
    "        aa_idx = action % 20\n",
    "        aa = AMINO_ACIDS[aa_idx]\n",
    "        pos = min(max(pos, 0), max(0, len(self._pep)-1))\n",
    "        new_pep = self._pep[:pos] + aa + self._pep[pos+1:]\n",
    "        new_score = self._predict(new_pep)\n",
    "        reward = new_score - self._current_score  # delta improvement\n",
    "        self._pep = new_pep\n",
    "        self._current_score = new_score\n",
    "        self._steps += 1\n",
    "        done = (self._steps >= self.max_steps)\n",
    "        return self._obs(self._pep), reward, done, {\"peptide\": self._pep, \"score\": new_score}\n",
    "\n",
    "# -------------------------------\n",
    "# PPO\n",
    "# -------------------------------\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden: int=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden), nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden), nn.Tanh(),\n",
    "            nn.Linear(hidden, action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.distributions.Categorical(logits=self.net(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, hidden: int=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden), nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden), nn.Tanh(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "@dataclass\n",
    "class PPOConfig:\n",
    "    epochs: int = 10\n",
    "    steps_per_epoch: int = 2048\n",
    "    gamma: float = 0.99\n",
    "    lam: float = 0.95\n",
    "    clip_ratio: float = 0.2\n",
    "    lr_actor: float = 3e-4\n",
    "    lr_critic: float = 1e-3\n",
    "    train_iters: int = 10\n",
    "    max_grad_norm: float = 0.5\n",
    "\n",
    "def compute_gae(rews, vals, dones, gamma, lam):\n",
    "    T = len(rews)\n",
    "    adv = torch.zeros(T, device=DEVICE)\n",
    "    lastgaelam = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        nextnonterminal = 1.0 - dones[t]\n",
    "        nextvalue = vals[t+1] if t+1 < len(vals) else 0.0\n",
    "        delta = rews[t] + gamma * nextvalue * nextnonterminal - vals[t]\n",
    "        lastgaelam = delta + gamma * lam * nextnonterminal * lastgaelam\n",
    "        adv[t] = lastgaelam\n",
    "    ret = adv + vals[:T]\n",
    "    return adv, ret\n",
    "\n",
    "def ppo_train(env: MutationEnv, seed_peptides: List[str], cfg: PPOConfig):\n",
    "    obs_dim = env.obs_dim\n",
    "    act_dim = env.action_space_n\n",
    "    actor = Actor(obs_dim, act_dim).to(DEVICE)\n",
    "    critic = Critic(obs_dim).to(DEVICE)\n",
    "    opt_a = optim.Adam(actor.parameters(), lr=cfg.lr_actor)\n",
    "    opt_c = optim.Adam(critic.parameters(), lr=cfg.lr_critic)\n",
    "\n",
    "    for ep in range(cfg.epochs):\n",
    "        obs_buf, act_buf, logp_buf, rew_buf, done_buf, val_buf = [], [], [], [], [], []\n",
    "        # rollouts across seed peptides\n",
    "        steps = 0\n",
    "        while steps < cfg.steps_per_epoch:\n",
    "            pep0 = random.choice(seed_peptides)\n",
    "            obs = env.reset(pep0)\n",
    "            done = False\n",
    "            while not done and steps < cfg.steps_per_epoch:\n",
    "                with torch.no_grad():\n",
    "                    pi = actor(obs)\n",
    "                    a = pi.sample()\n",
    "                    logp = pi.log_prob(a)\n",
    "                    v = critic(obs)\n",
    "                nobs, r, done, info = env.step(int(a.item()))\n",
    "                # store\n",
    "                obs_buf.append(obs)\n",
    "                act_buf.append(a)\n",
    "                logp_buf.append(logp)\n",
    "                rew_buf.append(torch.tensor([r], device=DEVICE))\n",
    "                done_buf.append(torch.tensor([float(done)], device=DEVICE))\n",
    "                val_buf.append(v)\n",
    "\n",
    "                obs = nobs\n",
    "                steps += 1\n",
    "\n",
    "        # stack\n",
    "        obs_b = torch.cat(obs_buf, dim=0)\n",
    "        act_b = torch.stack(act_buf).squeeze(-1)\n",
    "        logp_b = torch.stack(logp_buf).squeeze(-1)\n",
    "        rew_b = torch.cat(rew_buf).squeeze(-1)\n",
    "        done_b = torch.cat(done_buf).squeeze(-1)\n",
    "        val_b = torch.stack(val_buf).squeeze(-1)\n",
    "\n",
    "        # bootstrap last value\n",
    "        with torch.no_grad():\n",
    "            last_v = critic(obs_b[-1].unsqueeze(0))\n",
    "        vals_ext = torch.cat([val_b, last_v.reshape(1)])\n",
    "\n",
    "        adv, ret = compute_gae(rew_b, vals_ext, done_b, cfg.gamma, cfg.lam)\n",
    "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "        # optimize\n",
    "        for _ in range(cfg.train_iters):\n",
    "            pi = actor(obs_b)\n",
    "            logp = pi.log_prob(act_b)\n",
    "            ratio = torch.exp(logp - logp_b)\n",
    "            surr1 = ratio * adv\n",
    "            surr2 = torch.clamp(ratio, 1 - cfg.clip_ratio, 1 + cfg.clip_ratio) * adv\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * pi.entropy().mean()\n",
    "\n",
    "            v = critic(obs_b)\n",
    "            critic_loss = ((v - ret) ** 2).mean()\n",
    "\n",
    "            opt_a.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(actor.parameters(), cfg.max_grad_norm)\n",
    "            opt_a.step()\n",
    "\n",
    "            opt_c.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(critic.parameters(), cfg.max_grad_norm)\n",
    "            opt_c.step()\n",
    "\n",
    "        print(f\"[PPO] epoch {ep+1}/{cfg.epochs}  actor_loss={actor_loss.item():.4f}  critic_loss={critic_loss.item():.4f}\")\n",
    "\n",
    "    return actor, critic\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation metrics\n",
    "# -------------------------------\n",
    "def evaluate_model(model: ImmuneResponseModel, loader: DataLoader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for pep, mhc, y in loader:\n",
    "            y_hat = model(pep.to(DEVICE), mhc.to(DEVICE)).cpu().numpy().reshape(-1)\n",
    "            y_true.extend(y.numpy().reshape(-1))\n",
    "            y_pred.extend(y_hat)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    try:\n",
    "        pearson, _ = pearsonr(y_true, y_pred)\n",
    "    except Exception:\n",
    "        pearson = float('nan')\n",
    "    return {\"MSE\": mse, \"RMSE\": rmse, \"R2\": r2, \"Pearson\": pearson}\n",
    "\n",
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "def main():\n",
    "    set_seed(123)\n",
    "    train_path = \"train_BA1.txt\"\n",
    "    test_path  = \"test_BA1.txt\"\n",
    "    if not (os.path.exists(train_path) and os.path.exists(test_path)):\n",
    "        raise FileNotFoundError(\"Expected train_BA1.txt and test_BA1.txt under /mnt/data/\")\n",
    "\n",
    "    train_df, test_df = load_dataset(train_path, test_path)\n",
    "\n",
    "    # data loaders\n",
    "    tr_ds = PMHCDataset(train_df)\n",
    "    te_ds = PMHCDataset(test_df)\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=64, shuffle=True)\n",
    "    te_loader = DataLoader(te_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "    # supervised model\n",
    "    model = ImmuneResponseModel().to(DEVICE)\n",
    "    train_supervised(model, tr_loader, te_loader, epochs=12, lr=1e-3)\n",
    "\n",
    "    # evaluation\n",
    "    metrics = evaluate_model(model, te_loader)\n",
    "    print(\"\\n== Supervised Test Metrics ==\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.6f}\")\n",
    "\n",
    "    # RL environment with a default MHC from the test set\n",
    "    default_mhc = str(test_df[\"mhc_seq\"].iloc[0]) if len(test_df) > 0 else \"HLA-DPA10103-DPB10201\"\n",
    "    env = MutationEnv(model, mhc_seq=default_mhc, max_steps=8)\n",
    "\n",
    "    # seed peptides from test set\n",
    "    seed_peptides = test_df[\"peptide\"].astype(str).sample(min(32, len(test_df)), random_state=123).tolist()\n",
    "\n",
    "    # train PPO\n",
    "    ppo_cfg = PPOConfig(epochs=6, steps_per_epoch=1024, train_iters=8)\n",
    "    actor, critic = ppo_train(env, seed_peptides, ppo_cfg)\n",
    "\n",
    "    # demonstrate improvements for a few peptides\n",
    "    print(\"\\n== PPO Mutation Demonstration ==\")\n",
    "    for pep in seed_peptides[:5]:\n",
    "        obs = env.reset(pep)\n",
    "        best = {\"pep\": pep, \"score\": env._current_score}\n",
    "        for _ in range(env.max_steps):\n",
    "            with torch.no_grad():\n",
    "                pi = actor(obs)\n",
    "                a = pi.probs.argmax()  # greedy for demo\n",
    "            obs, r, done, info = env.step(int(a.item()))\n",
    "            if info[\"score\"] > best[\"score\"]:\n",
    "                best = {\"pep\": info[\"peptide\"], \"score\": info[\"score\"]}\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"Start: {pep} | Best mutant: {best['pep']} | Pred cytokine: {best['score']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b0dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
