{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b138818",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training ImmuneNet (fixed convex) ===\n",
      "[Epoch 1] Val MSE=0.0745, R2=-0.1134\n",
      "[Epoch 2] Val MSE=0.0640, R2=0.0430\n",
      "[Epoch 3] Val MSE=0.0638, R2=0.0462\n",
      "[Epoch 4] Val MSE=0.0622, R2=0.0707\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Immune RL with PPO + Gated ReLU (Multistep-Only) — With Improvement Tracking & Failure Presets\n",
    "-----------------------------------------------------------------------------------------------\n",
    "- Two data modes:\n",
    "    * peptide (default): peptide/TCR/allele inputs using ImmuneNet (sequence encoders)\n",
    "    * pbmc: PBMC10k multiome fused vectors using VectorNet (optional muon/mudatasets, PyG for GAT)\n",
    "- Trains base net once (fixed activation: convex or nonconvex) and reuses weights across PPO runs.\n",
    "- PPO actor/critic activation varies per run: convex, nonconvex, or twostage (convex→nonconvex switch).\n",
    "- Continuous cytokine control with safe random perturbations and multistep discounted returns.\n",
    "- Oversampling on TRAIN subset (optional): 'target_bin' or 'allele' (peptide mode only).\n",
    "- Tracks per-setup BEFORE vs AFTER metrics and writes deltas to performance_improvements.csv\n",
    "- Saves raw results to gated_relu_multistep.csv and optional per-step reward curves to ppo_multistep_rewards.csv\n",
    "- Optional presets to INDUCE local optima behavior for demonstration:\n",
    "    * --failure_preset convex_flat       → convex-only gets stuck in a flat, underfit basin\n",
    "    * --failure_preset nonconvex_sharp   → nonconvex-only gets trapped in a sharp local minimum\n",
    "\n",
    "Usage examples\n",
    "--------------\n",
    "# Peptide mode (default)\n",
    "python immune_rl_multistep_with_improvements.py \\\n",
    "  --train train_BA1.txt --test test_BA1.txt --alleles allelelist.txt \\\n",
    "  --immune_act_mode convex --ppo_modes convex,nonconvex,twostage \\\n",
    "  --epochs 8 --ppo_episodes 20 --log_step_rewards\n",
    "\n",
    "# PBMC mode (requires muon+mudatasets; falls back to raw features if PyG not installed)\n",
    "python immune_rl_multistep_with_improvements.py --data_mode pbmc --pbmc_sample 8000 \\\n",
    "  --immune_act_mode convex --ppo_modes twostage,nonconvex --epochs 5 --ppo_episodes 10\n",
    "\n",
    "# Demonstrate local optima (convex flat or nonconvex sharp)\n",
    "python immune_rl_multistep_with_improvements.py --failure_preset convex_flat --ppo_modes convex\n",
    "python immune_rl_multistep_with_improvements.py --failure_preset nonconvex_sharp --ppo_modes nonconvex\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, csv, argparse, random, warnings\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# =========================\n",
    "# Optional deps\n",
    "# =========================\n",
    "HAS_TORCH = True\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "except Exception:\n",
    "    HAS_TORCH = False\n",
    "\n",
    "HAS_PYG = False\n",
    "if HAS_TORCH:\n",
    "    try:\n",
    "        from torch_geometric.nn import GATConv\n",
    "        HAS_PYG = True\n",
    "    except Exception:\n",
    "        HAS_PYG = False\n",
    "\n",
    "HAS_MUON = False\n",
    "try:\n",
    "    import muon as mu\n",
    "    import mudatasets as mds\n",
    "    from scipy.sparse import csr_matrix\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    HAS_MUON = True\n",
    "except Exception:\n",
    "    HAS_MUON = False\n",
    "\n",
    "# =========================\n",
    "# Constants & utils\n",
    "# =========================\n",
    "AA_VOCAB = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "AA_TO_ID = {a: i + 1 for i, a in enumerate(AA_VOCAB)}\n",
    "CYTOKINES = [\"NONE\", \"IL2\", \"IFNG\", \"IL10\", \"TNFA\"]\n",
    "CYTOKINE_TO_ID = {c: i for i, c in enumerate(CYTOKINES)}\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    if HAS_TORCH:\n",
    "        torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Example:\n",
    "    peptide: str\n",
    "    allele: str\n",
    "    score: float\n",
    "    motif: Optional[str] = None\n",
    "    tcr: Optional[str] = None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# I/O + Tokenizer (peptide mode)\n",
    "# =========================\n",
    "\n",
    "def smart_read_table(path: str) -> List[List[str]]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", newline=\"\") as f:\n",
    "        sample = f.read(2048); f.seek(0)\n",
    "        import csv as _csv\n",
    "        try:\n",
    "            dialect = _csv.Sniffer().sniff(sample, delimiters=\"\\t,;\")\n",
    "        except Exception:\n",
    "            class dialect: delimiter = \",\"\n",
    "        reader = _csv.reader(f, dialect)\n",
    "        for row in reader:\n",
    "            if row: rows.append([c.strip() for c in row])\n",
    "    return rows\n",
    "\n",
    "\n",
    "def load_alleles(path: str) -> Dict[str, int]:\n",
    "    uniq = []\n",
    "    for r in smart_read_table(path):\n",
    "        for c in r:\n",
    "            for token in c.replace(\",\", \" \").split():\n",
    "                if token and token not in uniq:\n",
    "                    uniq.append(token)\n",
    "    return {a: i for i, a in enumerate(sorted(uniq))}\n",
    "\n",
    "\n",
    "def parse_examples(path: str, allele_to_id: Dict[str, int]) -> List[Example]:\n",
    "    rows = smart_read_table(path); exs = []\n",
    "    for r in rows:\n",
    "        if len(r) < 3: continue\n",
    "        pep, score_str, allele = r[0], r[1], r[2]\n",
    "        try:\n",
    "            score = float(score_str)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if allele not in allele_to_id:\n",
    "            allele_to_id[allele] = len(allele_to_id)\n",
    "        exs.append(Example(peptide=pep, allele=allele, score=score))\n",
    "    return exs\n",
    "\n",
    "\n",
    "class SeqTokenizer:\n",
    "    def __init__(self, max_len=32): self.max_len = max_len\n",
    "\n",
    "    def encode_ids(self, s: str):\n",
    "        ids = [AA_TO_ID.get(ch, 0) for ch in s[:self.max_len]]\n",
    "        if len(ids) < self.max_len: ids += [0]*(self.max_len - len(ids))\n",
    "        return ids\n",
    "\n",
    "    def encode(self, s: str):\n",
    "        arr = np.asarray(self.encode_ids(s), dtype=np.int64)\n",
    "        return torch.tensor(arr, dtype=torch.long) if HAS_TORCH else arr\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Loss + Metrics\n",
    "# =========================\n",
    "\n",
    "def nonconvex_loss(pred, target, eps: float = 1e-6):\n",
    "    e = pred - target\n",
    "    return torch.mean(torch.sqrt(torch.abs(e) + eps))\n",
    "\n",
    "\n",
    "def regression_metrics(preds: np.ndarray, targets: np.ndarray) -> Dict[str, float]:\n",
    "    preds = np.asarray(preds).flatten()\n",
    "    targets = np.asarray(targets).flatten()\n",
    "    mse = float(np.mean((preds - targets) ** 2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae = float(np.mean(np.abs(preds - targets)))\n",
    "    denom = float(np.sum((targets - np.mean(targets)) ** 2) + 1e-12)\n",
    "    r2 = float(1.0 - np.sum((targets - preds) ** 2) / denom)\n",
    "    pear = float(pearsonr(preds, targets)[0]) if len(preds) > 1 and np.std(preds) > 0 and np.std(targets) > 0 else 0.0\n",
    "    return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2, \"Pearson\": pear}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Gated ReLU + Encoders\n",
    "# =========================\n",
    "if HAS_TORCH:\n",
    "    class GatedReLU(nn.Module):\n",
    "        \"\"\"\n",
    "        Modes:\n",
    "          - convex:    max(a*x + b, x)\n",
    "          - nonconvex: min(a*x + b, x)\n",
    "          - twostage:  start convex, switch to nonconvex after switch_epoch\n",
    "        \"\"\"\n",
    "        def __init__(self, dim: int, mode=\"convex\", switch_epoch=5):\n",
    "            super().__init__()\n",
    "            self.mode = mode\n",
    "            self.switch_epoch = switch_epoch\n",
    "            self.switched = False\n",
    "            self.a = nn.Parameter(torch.ones(dim))\n",
    "            self.b = nn.Parameter(torch.zeros(dim))\n",
    "            self.register_buffer(\"_epoch\", torch.zeros(1, dtype=torch.long), persistent=False)\n",
    "        def set_epoch(self, epoch: int): self._epoch[0] = epoch\n",
    "        def trigger_switch(self):\n",
    "            if not self.switched:\n",
    "                print(f\"⚡ GatedReLU switched to nonconvex at step {self._epoch.item()}\")\n",
    "                self.switched = True\n",
    "        def _convex(self, x): return torch.max(self.a * x + self.b, x)\n",
    "        def _nonconvex(self, x): return torch.min(self.a * x + self.b, x)\n",
    "        def forward(self, x):\n",
    "            if self.mode == \"convex\": return self._convex(x)\n",
    "            elif self.mode == \"nonconvex\": return self._nonconvex(x)\n",
    "            elif self.mode == \"twostage\":\n",
    "                if self.switched or (self._epoch.item() >= self.switch_epoch):\n",
    "                    return self._nonconvex(x)\n",
    "                else:\n",
    "                    return self._convex(x)\n",
    "            else: raise ValueError(f\"Unknown mode: {self.mode}\")\n",
    "\n",
    "    class MiniGAT(nn.Module):\n",
    "        def __init__(self, vocab_size, dim, max_len=32, heads=4, layers=2):\n",
    "            super().__init__()\n",
    "            self.emb = nn.Embedding(vocab_size+1, dim)\n",
    "            self.pos = nn.Embedding(max_len, dim)\n",
    "            enc = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, batch_first=True)\n",
    "            self.encoder = nn.TransformerEncoder(enc, num_layers=layers)\n",
    "            self.max_len = max_len\n",
    "        def forward(self, x):\n",
    "            L = min(x.size(1), self.max_len)\n",
    "            pos = torch.arange(L, device=x.device).unsqueeze(0).expand(x.size(0), L)\n",
    "            h = self.emb(x[:, :L]) + self.pos(pos)\n",
    "            h = self.encoder(h)\n",
    "            mean, cls = h.mean(dim=1), h[:, 0, :]\n",
    "            return torch.cat([mean, cls], dim=-1)\n",
    "\n",
    "    class ImmuneNet(nn.Module):\n",
    "        \"\"\"Sequence/TCR/Allele encoder (peptide mode).\"\"\"\n",
    "        def __init__(self, vocab_size, allele_count, dim=128, pep_len=32, tcr_len=24,\n",
    "                     act_mode=\"convex\"):\n",
    "            super().__init__()\n",
    "            self.pep_enc = MiniGAT(vocab_size, dim, pep_len)\n",
    "            self.tcr_enc = MiniGAT(vocab_size, dim, tcr_len)\n",
    "            self.all_emb = nn.Embedding(allele_count+1, dim)\n",
    "            in_dim = 2*dim + 2*dim + dim  # pep(mean+cls) + tcr(mean+cls) + allele\n",
    "            hid = 256\n",
    "            def act(): return GatedReLU(hid, act_mode)\n",
    "            self.backbone = nn.Sequential(nn.Linear(in_dim, hid), act(),\n",
    "                                          nn.Linear(hid, hid), act())\n",
    "            self.binding = nn.Linear(hid, 1)\n",
    "            self.recognition = nn.Linear(hid, 1)\n",
    "            self.cyt_fc = nn.Linear(len(CYTOKINES), 32)\n",
    "            self.response = nn.Sequential(nn.Linear(hid+32, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "        def encode_backbone(self, pep, tcr, allele):\n",
    "            pep_h = self.pep_enc(pep)\n",
    "            tcr_h = self.tcr_enc(tcr)\n",
    "            all_h = self.all_emb(allele)\n",
    "            return self.backbone(torch.cat([pep_h, tcr_h, all_h], dim=-1))\n",
    "        def forward(self, pep, tcr, allele, cytok_onehot):\n",
    "            z = self.encode_backbone(pep, tcr, allele)\n",
    "            bind = torch.sigmoid(self.binding(z))\n",
    "            recog = torch.sigmoid(self.recognition(z))\n",
    "            c = F.relu(self.cyt_fc(cytok_onehot))\n",
    "            resp = self.response(torch.cat([z, c], dim=-1))\n",
    "            return bind, recog, resp\n",
    "\n",
    "    class VectorNet(nn.Module):\n",
    "        \"\"\"Vector feature encoder (PBMC mode).\"\"\"\n",
    "        def __init__(self, in_dim, act_mode=\"convex\"):\n",
    "            super().__init__()\n",
    "            hid = 256\n",
    "            def act(): return GatedReLU(hid, act_mode)\n",
    "            self.backbone = nn.Sequential(nn.Linear(in_dim, hid), act(),\n",
    "                                          nn.Linear(hid, hid), act())\n",
    "            self.binding = nn.Linear(hid, 1)\n",
    "            self.recognition = nn.Linear(hid, 1)\n",
    "            self.cyt_fc = nn.Linear(len(CYTOKINES), 32)\n",
    "            self.response = nn.Sequential(nn.Linear(hid+32, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "        def encode_backbone(self, x_vec):\n",
    "            return self.backbone(x_vec)\n",
    "        def forward(self, x_vec, cytok_onehot):\n",
    "            z = self.encode_backbone(x_vec)\n",
    "            bind = torch.sigmoid(self.binding(z))\n",
    "            recog = torch.sigmoid(self.recognition(z))\n",
    "            c = F.relu(self.cyt_fc(cytok_onehot))\n",
    "            resp = self.response(torch.cat([z, c], dim=-1))\n",
    "            return bind, recog, resp\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PPO: continuous cytokine env (multistep-only)\n",
    "# =========================\n",
    "if HAS_TORCH:\n",
    "    class PPOPolicy(nn.Module):\n",
    "        def __init__(self, input_dim, num_actions, hidden=256, act_mode=\"convex\", switch_epoch=5):\n",
    "            super().__init__()\n",
    "            def act(): return GatedReLU(hidden, act_mode, switch_epoch)\n",
    "            self.actor = nn.Sequential(nn.Linear(input_dim, hidden), act(), nn.Linear(hidden, num_actions))\n",
    "            self.critic = nn.Sequential(nn.Linear(input_dim, hidden), act(), nn.Linear(hidden, 1))\n",
    "            self.act_mode = act_mode; self.switch_epoch = switch_epoch\n",
    "        def set_epoch(self, epoch: int):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, GatedReLU):\n",
    "                    m.set_epoch(epoch)\n",
    "                    if self.act_mode == \"twostage\" and not m.switched and epoch >= self.switch_epoch:\n",
    "                        m.trigger_switch()\n",
    "        def act(self, x):\n",
    "            mu = self.actor(x)\n",
    "            dist = torch.distributions.Normal(mu, 0.1)\n",
    "            a = dist.sample()\n",
    "            logp = dist.log_prob(a).sum(-1)\n",
    "            v = self.critic(x).squeeze(-1)\n",
    "            return torch.tanh(a), logp, v\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_state_seq(model: ImmuneNet, pep, tcr, allele):\n",
    "        return model.encode_backbone(pep, tcr, allele).detach()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_state_vec(model: VectorNet, x_vec):\n",
    "        return model.encode_backbone(x_vec).detach()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def env_step_seq(model, pep, tcr, allele, action_cont, device,\n",
    "                     perturb_prob=0.2, action_mag=0.4, max_spike=3, cytok_prev: Optional[torch.Tensor]=None):\n",
    "        B = pep.size(0); N = len(CYTOKINES)\n",
    "        if cytok_prev is None:\n",
    "            cytok = torch.zeros((B, N), device=device); cytok[:, CYTOKINE_TO_ID[\"NONE\"]] = 1.0\n",
    "        else:\n",
    "            cytok = cytok_prev.clone()\n",
    "        for i in range(B):\n",
    "            if random.random() < perturb_prob:\n",
    "                k = np.random.randint(1, min(max_spike, N) + 1)\n",
    "                idx = np.random.choice(N, size=k, replace=False)\n",
    "                signs = np.random.choice([-1.0, 1.0], size=k)\n",
    "                cytok[i, idx] += action_mag * torch.tensor(signs, dtype=torch.float32, device=device)\n",
    "        cytok_next = torch.clamp(cytok + 0.1 * action_cont, 0.0, 1.0)\n",
    "        _, recog, resp = model(pep, tcr, allele, cytok_next)\n",
    "        reward = (0.7*resp + 0.3*recog).squeeze(-1).detach()\n",
    "        return reward, cytok_next\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def env_step_vec(model, x_vec, action_cont, device,\n",
    "                     perturb_prob=0.2, action_mag=0.4, max_spike=3, cytok_prev: Optional[torch.Tensor]=None):\n",
    "        B = x_vec.size(0); N = len(CYTOKINES)\n",
    "        if cytok_prev is None:\n",
    "            cytok = torch.zeros((B, N), device=device); cytok[:, CYTOKINE_TO_ID[\"NONE\"]] = 1.0\n",
    "        else:\n",
    "            cytok = cytok_prev.clone()\n",
    "        for i in range(B):\n",
    "            if random.random() < perturb_prob:\n",
    "                k = np.random.randint(1, min(max_spike, N) + 1)\n",
    "                idx = np.random.choice(N, size=k, replace=False)\n",
    "                signs = np.random.choice([-1.0, 1.0], size=k)\n",
    "                cytok[i, idx] += action_mag * torch.tensor(signs, dtype=torch.float32, device=device)\n",
    "        cytok_next = torch.clamp(cytok + 0.1 * action_cont, 0.0, 1.0)\n",
    "        _, recog, resp = model(x_vec, cytok_next)\n",
    "        reward = (0.7*resp + 0.3*recog).squeeze(-1).detach()\n",
    "        return reward, cytok_next\n",
    "\n",
    "    def ppo_train_multistep(\n",
    "        state_dim: int,\n",
    "        make_state_batch,       # function() -> dict with 'state', 'cytok_init', ... or None when epoch ends\n",
    "        env_step_fn,            # function(batch, action, device, cytok_prev=None) -> (reward, next_cytok)\n",
    "        policy_act_mode=\"convex\",\n",
    "        switch_epoch=5,\n",
    "        ppo_epochs=3, episodes=5, clip=0.2,\n",
    "        discount_gamma=0.99, multistep_steps=50,\n",
    "        lr=5e-4,\n",
    "        log_step_rewards=False, step_log_writer: Optional[csv.writer]=None, setup_name: str=\"\"\n",
    "    ):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        policy = PPOPolicy(state_dim, len(CYTOKINES), hidden=256,\n",
    "                           act_mode=policy_act_mode, switch_epoch=switch_epoch).to(device)\n",
    "        opt = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "        for ep in range(episodes):\n",
    "            policy.set_epoch(ep)\n",
    "            S_list, A_list, oldlogp_list, G_list, Adv_list = [], [], [], [], []\n",
    "            step_rewards_accum = np.zeros(multistep_steps, dtype=np.float64)\n",
    "            n_batches = 0\n",
    "\n",
    "            while True:\n",
    "                batch = make_state_batch()\n",
    "                if batch is None:\n",
    "                    break\n",
    "                z = batch[\"state\"].to(device)\n",
    "                cytok_init = batch[\"cytok_init\"].to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    gamma = discount_gamma\n",
    "                    discounted = torch.zeros(z.size(0), device=device)\n",
    "                    cytok_state = cytok_init\n",
    "                    per_step_rewards = []\n",
    "                    for t in range(multistep_steps):\n",
    "                        a, logp, v = policy.act(z)\n",
    "                        r_t, cytok_state = env_step_fn(batch, a, device, cytok_prev=cytok_state)\n",
    "                        discounted += (gamma**t) * r_t\n",
    "                        per_step_rewards.append(r_t.mean().item())\n",
    "                    a_last, logp_last, v_last = policy.act(z)\n",
    "                    G = discounted\n",
    "                    A = (G - v_last).detach()\n",
    "\n",
    "                S_list.append(z); A_list.append(a_last); oldlogp_list.append(logp_last); G_list.append(G); Adv_list.append(A)\n",
    "                step_rewards_accum += np.array(per_step_rewards, dtype=np.float64)\n",
    "                n_batches += 1\n",
    "\n",
    "            if not S_list:\n",
    "                print(\"No PPO batches produced this episode.\")\n",
    "                continue\n",
    "\n",
    "            S  = torch.cat(S_list)\n",
    "            A  = torch.cat(A_list)\n",
    "            OL = torch.cat(oldlogp_list)\n",
    "            Gt = torch.cat(G_list)\n",
    "            Adv= torch.cat(Adv_list)\n",
    "            Adv = (Adv - Adv.mean()) / (Adv.std() + 1e-8)\n",
    "\n",
    "            for _ in range(ppo_epochs):\n",
    "                mu = policy.actor(S)\n",
    "                dist = torch.distributions.Normal(mu, 0.1)\n",
    "                new_logp = dist.log_prob(A).sum(-1)\n",
    "                ratio = torch.exp(new_logp - OL)\n",
    "                surr1 = ratio * Adv\n",
    "                surr2 = torch.clamp(ratio, 1.0 - clip, 1.0 + clip) * Adv\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                v_pred = policy.critic(S).squeeze(-1)\n",
    "                critic_loss = nonconvex_loss(v_pred, Gt)\n",
    "\n",
    "                loss = actor_loss + 0.5 * critic_loss\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            print(f\"[PPO] Episode {ep+1}/{episodes} (multistep={multistep_steps}): return_mean={Gt.mean().item():.4f}\")\n",
    "            if log_step_rewards and step_log_writer is not None and n_batches > 0:\n",
    "                curve = (step_rewards_accum / max(n_batches,1)).tolist()\n",
    "                for tstep, rmean in enumerate(curve):\n",
    "                    step_log_writer.writerow([setup_name, ep+1, tstep+1, rmean])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Datasets + loaders\n",
    "# =========================\n",
    "if HAS_TORCH:\n",
    "    # ----- Peptide dataset (original) -----\n",
    "    class PeptideDataset(Dataset):\n",
    "        def __init__(self, examples, allele_to_id, pep_len=32, tcr_len=24):\n",
    "            self.examples = examples; self.allele_to_id = allele_to_id\n",
    "            self.tok_p = SeqTokenizer(pep_len); self.tok_t = SeqTokenizer(tcr_len)\n",
    "        def __len__(self): return len(self.examples)\n",
    "        def __getitem__(self, idx):\n",
    "            ex = self.examples[idx]\n",
    "            pep = self.tok_p.encode(ex.peptide)\n",
    "            tcr = self.tok_t.encode(ex.tcr or \"CASSIRSSYEQYF\")\n",
    "            all_idx = self.allele_to_id.get(ex.allele, 0)\n",
    "            return pep, tcr, all_idx, float(ex.score)\n",
    "\n",
    "    def collate_pep(batch):\n",
    "        pep, tcr, all_idx, y = zip(*batch)\n",
    "        pep = torch.stack(pep); tcr = torch.stack(tcr)\n",
    "        all_idx = torch.tensor(all_idx, dtype=torch.long)\n",
    "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)\n",
    "        cytok = torch.zeros((pep.size(0), len(CYTOKINES)), dtype=torch.float32)\n",
    "        cytok[:, CYTOKINE_TO_ID[\"NONE\"]] = 1.0\n",
    "        return pep, tcr, all_idx, cytok, y\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_loader_metrics_seq(model: ImmuneNet, loader, device):\n",
    "        model.eval(); preds, targets = [], []\n",
    "        for pep, tcr, all_idx, cytok, y in loader:\n",
    "            pep, tcr, all_idx, cytok = pep.to(device), tcr.to(device), all_idx.to(device), cytok.to(device)\n",
    "            bind, _, _ = model(pep, tcr, all_idx, cytok)\n",
    "            preds.extend(bind.cpu().numpy()); targets.extend(y.numpy())\n",
    "        return regression_metrics(np.array(preds), np.array(targets))\n",
    "\n",
    "    def train_supervised_seq(model: ImmuneNet, train_loader, val_loader, device, epochs=10, lr=1e-3, weight_decay=0.0, betas=(0.9,0.999)):\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas=betas)\n",
    "        for ep in range(epochs):\n",
    "            model.train()\n",
    "            for pep, tcr, all_idx, cytok, y in train_loader:\n",
    "                pep, tcr, all_idx, cytok, y = pep.to(device), tcr.to(device), all_idx.to(device), cytok.to(device), y.to(device)\n",
    "                bind, _, _ = model(pep, tcr, all_idx, cytok)\n",
    "                loss = nonconvex_loss(bind, y)\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "            vm = eval_loader_metrics_seq(model, val_loader, device)\n",
    "            print(f\"[Epoch {ep+1}] Val MSE={vm['MSE']:.4f}, R2={vm['R2']:.4f}\")\n",
    "\n",
    "    # ----- PBMC vector dataset -----\n",
    "    class VectorDataset(Dataset):\n",
    "        def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "            self.X = X.astype(np.float32)\n",
    "            self.y = y.astype(np.float32).reshape(-1, 1)\n",
    "        def __len__(self): return self.X.shape[0]\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    def collate_vec(batch):\n",
    "        X, y = zip(*batch)\n",
    "        X = torch.tensor(np.stack(X), dtype=torch.float32)\n",
    "        y = torch.tensor(np.stack(y), dtype=torch.float32)\n",
    "        cytok = torch.zeros((X.size(0), len(CYTOKINES)), dtype=torch.float32)\n",
    "        cytok[:, CYTOKINE_TO_ID[\"NONE\"]] = 1.0\n",
    "        return X, cytok, y\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_loader_metrics_vec(model: VectorNet, loader, device):\n",
    "        model.eval(); preds, targets = [], []\n",
    "        for X, cytok, y in loader:\n",
    "            X, cytok = X.to(device), cytok.to(device)\n",
    "            bind, _, _ = model(X, cytok)\n",
    "            preds.extend(bind.cpu().numpy()); targets.extend(y.numpy())\n",
    "        return regression_metrics(np.array(preds), np.array(targets))\n",
    "\n",
    "    def train_supervised_vec(model: VectorNet, train_loader, val_loader, device, epochs=10, lr=1e-3, weight_decay=0.0, betas=(0.9,0.999)):\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas=betas)\n",
    "        for ep in range(epochs):\n",
    "            model.train()\n",
    "            for X, cytok, y in train_loader:\n",
    "                X, cytok, y = X.to(device), cytok.to(device), y.to(device)\n",
    "                bind, _, _ = model(X, cytok)\n",
    "                loss = nonconvex_loss(bind, y)\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "            vm = eval_loader_metrics_vec(model, val_loader, device)\n",
    "            print(f\"[Epoch {ep+1}] Val MSE={vm['MSE']:.4f}, R2={vm['R2']:.4f}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Oversampling helpers (peptide)\n",
    "# =========================\n",
    "if HAS_TORCH:\n",
    "    def _extract_examples_from_subset(ds_or_subset):\n",
    "        if isinstance(ds_or_subset, Subset):\n",
    "            base: PeptideDataset = ds_or_subset.dataset\n",
    "            return [base.examples[i] for i in ds_or_subset.indices]\n",
    "        elif isinstance(ds_or_subset, PeptideDataset):\n",
    "            return ds_or_subset.examples\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported dataset type for oversampling.\")\n",
    "\n",
    "    def build_weighted_sampler(\n",
    "        ds_or_subset,\n",
    "        allele_to_id: Dict[str, int],\n",
    "        by: str = \"target_bin\",\n",
    "        n_bins: int = 6,\n",
    "        min_count_smoothing: float = 1.0\n",
    "    ):\n",
    "        exs = _extract_examples_from_subset(ds_or_subset)\n",
    "        N = len(exs)\n",
    "        if N == 0: return None\n",
    "        weights = np.ones(N, dtype=np.float32)\n",
    "        if by == \"allele\":\n",
    "            allele_ids = np.array([allele_to_id.get(ex.allele, 0) for ex in exs])\n",
    "            uniq, counts = np.unique(allele_ids, return_counts=True)\n",
    "            freq = {u: c for u, c in zip(uniq, counts)}\n",
    "            weights = np.array([1.0 / (freq[a] + min_count_smoothing) for a in allele_ids], dtype=np.float32)\n",
    "        else:\n",
    "            targets = np.array([ex.score for ex in exs], dtype=np.float32)\n",
    "            if np.allclose(targets.min(), targets.max()):\n",
    "                weights = np.ones(N, dtype=np.float32)\n",
    "            else:\n",
    "                quantiles = np.linspace(0, 1, num=n_bins+1)\n",
    "                edges = np.quantile(targets, quantiles)\n",
    "                edges = np.unique(edges)\n",
    "                if len(edges) < 3:\n",
    "                    edges = np.linspace(targets.min(), targets.max(), num=max(3, n_bins+1))\n",
    "                bins = np.clip(np.digitize(targets, edges[1:-1], right=False), 0, len(edges)-2)\n",
    "                uniq, counts = np.unique(bins, return_counts=True)\n",
    "                freq = {u: c for u, c in zip(uniq, counts)}\n",
    "                weights = np.array([1.0 / (freq[b] + min_count_smoothing) for b in bins], dtype=np.float32)\n",
    "        weights = weights / weights.sum()\n",
    "        weights = torch.tensor(weights, dtype=torch.double)\n",
    "        sampler = WeightedRandomSampler(weights, num_samples=len(exs), replacement=True)\n",
    "        return sampler\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PBMC helpers (optional)\n",
    "# =========================\n",
    "\n",
    "def compute_knn_edges(X: np.ndarray, k=3):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='cosine').fit(X)\n",
    "    _, indices = nbrs.kneighbors(X)\n",
    "    edges = []\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in indices[i]:\n",
    "            if i != j: edges.append([i, j])\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "class SimpleGAT(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64, out_dim=32, heads=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        if not HAS_PYG:\n",
    "            raise RuntimeError(\"torch_geometric is required for SimpleGAT.\")\n",
    "        self.gat1 = GATConv(in_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.gat2 = GATConv(hidden_dim*heads, out_dim, heads=1, dropout=dropout)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index); x = torch.relu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "def pbmc_build_embeddings(sample_size=10000, seed=42):\n",
    "    if not HAS_MUON:\n",
    "        raise RuntimeError(\"muon/mudatasets is required for --data_mode pbmc.\")\n",
    "    np.random.seed(seed)\n",
    "    mdata = mds.load(\"pbmc10k_multiome\", full=True)\n",
    "    mdata.var_names_make_unique()\n",
    "    rna = mdata.mod['rna'].copy()\n",
    "    atac = mdata.mod['atac'].copy()\n",
    "    adt = mdata.mod['adt'].copy() if 'adt' in mdata.mod else None\n",
    "    common = rna.obs_names.intersection(atac.obs_names)\n",
    "    if adt is not None:\n",
    "        common = common.intersection(adt.obs_names)\n",
    "    common = np.array(common)\n",
    "    if sample_size < len(common):\n",
    "        sel = np.random.choice(common, size=sample_size, replace=False)\n",
    "    else:\n",
    "        sel = common\n",
    "    rna = rna[sel]; atac = atac[sel];\n",
    "    if adt is not None: adt = adt[sel]\n",
    "    rna_X = rna.X.toarray() if hasattr(rna.X, \"toarray\") else np.asarray(rna.X)\n",
    "    atac_X = atac.X.toarray() if hasattr(atac.X, \"toarray\") else np.asarray(atac.X)\n",
    "    adt_X = adt.X.toarray() if adt is not None and hasattr(adt.X, \"toarray\") else (adt.X if adt is not None else None)\n",
    "\n",
    "    def make_emb(X):\n",
    "        if X is None: return None\n",
    "        if HAS_PYG:\n",
    "            X_t = torch.tensor(X, dtype=torch.float32)\n",
    "            edge_index = compute_knn_edges(X, k=3)\n",
    "            model = SimpleGAT(in_dim=X.shape[1])\n",
    "            with torch.no_grad():\n",
    "                Z = model(X_t, edge_index).cpu().numpy()\n",
    "            return Z\n",
    "        else:\n",
    "            return X  # fallback: raw features if PyG not available\n",
    "\n",
    "    z_rna = make_emb(rna_X)\n",
    "    z_atac = make_emb(atac_X)\n",
    "    z_adt = make_emb(adt_X) if adt_X is not None else None\n",
    "\n",
    "    scalers = {}\n",
    "    expr_train, expr_test, pseudo_train, pseudo_test = {}, {}, {}, {}\n",
    "    mods = {\"rna\": z_rna, \"atac\": z_atac}\n",
    "    if z_adt is not None: mods[\"adt\"] = z_adt\n",
    "    for mod, Z in mods.items():\n",
    "        scaler = StandardScaler()\n",
    "        Zs = scaler.fit_transform(Z); scalers[mod] = scaler\n",
    "        idx = np.arange(Zs.shape[0])\n",
    "        tr, te = train_test_split(idx, test_size=0.2, random_state=seed)\n",
    "        expr_train[mod] = Zs[tr]; expr_test[mod] = Zs[te]\n",
    "        pseudo = np.linspace(0.0, 1.0, Zs.shape[0]).astype(np.float32)\n",
    "        pseudo_train[mod] = pseudo[tr]; pseudo_test[mod] = pseudo[te]\n",
    "\n",
    "    fused_train = np.concatenate([expr_train[m] for m in expr_train.keys()], axis=1)\n",
    "    fused_test  = np.concatenate([expr_test[m]  for m in expr_test.keys()],  axis=1)\n",
    "    pt_train = np.mean(np.stack([pseudo_train[m] for m in pseudo_train.keys()], axis=1), axis=1)\n",
    "    pt_test  = np.mean(np.stack([pseudo_test[m]  for m in pseudo_test.keys()],  axis=1), axis=1)\n",
    "\n",
    "    return fused_train, pt_train, fused_test, pt_test\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Adaptive per-gene threshold (optional hook)\n",
    "# =========================\n",
    "class PerGeneAdaptiveThreshold:\n",
    "    def __init__(self, modality_dims: Dict[str,int], alpha=0.1):\n",
    "        self.thresholds = {mod: {i: 0.0 for i in range(dim)} for mod, dim in modality_dims.items()}\n",
    "        self.alpha = alpha\n",
    "    def update(self, gene_rewards: Dict[str, Dict[int, float]]):\n",
    "        for mod, rewards in gene_rewards.items():\n",
    "            for gid, r in rewards.items():\n",
    "                if r is None or (isinstance(r, float) and np.isnan(r)): continue\n",
    "                self.thresholds[mod][gid] = self.alpha*float(r) + (1-self.alpha)*self.thresholds[mod].get(gid,0.0)\n",
    "    def get(self, mod, gene_id):\n",
    "        return float(self.thresholds.get(mod, {}).get(gene_id,0.0))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    import sys\n",
    "    if any(a.startswith(\"-f\") or \"kernel\" in a for a in sys.argv):\n",
    "        sys.argv = [sys.argv[0]]\n",
    "\n",
    "    p = argparse.ArgumentParser()\n",
    "    # --- Data mode ---\n",
    "    p.add_argument('--data_mode', type=str, default='peptide', choices=['peptide','pbmc'],\n",
    "                   help='peptide: original peptide/TCR/allele; pbmc: PBMC10k multiome fused vectors')\n",
    "    # --- Peptide paths ---\n",
    "    p.add_argument('--train', default='train_BA1.txt')\n",
    "    p.add_argument('--test', default='test_BA1.txt')\n",
    "    p.add_argument('--alleles', default='allelelist.txt')\n",
    "    # --- Common training ---\n",
    "    p.add_argument('--epochs', type=int, default=8)\n",
    "    p.add_argument('--batch_size', type=int, default=64)\n",
    "    p.add_argument('--ppo_episodes', type=int, default=5)\n",
    "    p.add_argument('--ppo_epochs', type=int, default=10)\n",
    "    # --- Sequence lengths (peptide mode) ---\n",
    "    p.add_argument('--pep_len', type=int, default=32)\n",
    "    p.add_argument('--tcr_len', type=int, default=24)\n",
    "    # --- Immune/VectorNet activation (fixed) ---\n",
    "    p.add_argument('--immune_act_mode', type=str, default='convex', choices=['convex','nonconvex'],\n",
    "                   help='Fixed activation for base net across all PPO runs.')\n",
    "    # --- PPO activation modes (varies) ---\n",
    "    p.add_argument('--ppo_switch_epoch', type=int, default=4,\n",
    "                   help='Episode at which PPO two-stage switches to nonconvex.')\n",
    "    p.add_argument('--ppo_modes', type=str, default='convex,nonconvex,twostage',\n",
    "                   help='Comma-separated PPO activation modes to compare.')\n",
    "    # --- Cytokine env params ---\n",
    "    p.add_argument('--cytokine_perturb_prob', type=float, default=0.8,\n",
    "                   help='Prob. to randomly spike cytokines in env_step.')\n",
    "    p.add_argument('--cytokine_action_mag', type=float, default=0.8,\n",
    "                   help='Magnitude of random spike added when perturbing.')\n",
    "    p.add_argument('--cytokine_max_spike', type=int, default=5,\n",
    "                   help='Max cytokines to spike per perturbation (capped by N).')\n",
    "    # --- Oversampling (peptide only) ---\n",
    "    p.add_argument('--oversample', action='store_true',\n",
    "                   help='Enable inverse-frequency oversampling on the TRAIN subset (peptide mode).')\n",
    "    p.add_argument('--oversample_bins', type=int, default=6,\n",
    "                   help='Number of bins for target_bin oversampling.')\n",
    "    p.add_argument('--oversample_by', type=str, default='allele', choices=['target_bin','allele'],\n",
    "                   help='Oversample strategy: target_bin or allele.')\n",
    "    # --- Multistep PPO params ---\n",
    "    p.add_argument('--multistep_steps', type=int, default=5,\n",
    "                   help='Number of steps per PPO rollout (multistep only).')\n",
    "    p.add_argument('--discount_gamma', type=float, default=0.99,\n",
    "                   help='Discount factor for multistep returns.')\n",
    "    p.add_argument('--log_step_rewards', action='store_true',\n",
    "                   help='Write per-episode per-step reward means to ppo_multistep_rewards.csv')\n",
    "    # --- Optimizer knobs ---\n",
    "    p.add_argument('--base_lr', type=float, default=1e-3)\n",
    "    p.add_argument('--base_wd', type=float, default=0.0)\n",
    "    p.add_argument('--base_b1', type=float, default=0.9)\n",
    "    p.add_argument('--base_b2', type=float, default=0.999)\n",
    "    # --- PBMC controls ---\n",
    "    p.add_argument('--pbmc_sample', type=int, default=10000,\n",
    "                   help='PBMC sample size (if available cells fewer, uses all).')\n",
    "    p.add_argument('--val_ratio', type=float, default=0.2)\n",
    "    # --- Failure presets to induce local optima ---\n",
    "    p.add_argument('--failure_preset', type=str, default='none', choices=['none','convex_flat','nonconvex_sharp'],\n",
    "                   help='Set hyperparameters to encourage specific local-optima failures.')\n",
    "\n",
    "    a, _ = p.parse_known_args()\n",
    "    seed_everything(1)\n",
    "\n",
    "    if not HAS_TORCH:\n",
    "        print(\"⚠️ PyTorch not available.\"); return\n",
    "    d = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Apply failure presets (affects batch size, env noise, and base optimizer)\n",
    "    if a.failure_preset == 'convex_flat':\n",
    "        a.batch_size = max(128, a.batch_size)\n",
    "        a.cytokine_perturb_prob = 0.1\n",
    "        a.cytokine_action_mag = 0.2\n",
    "        a.base_lr = min(a.base_lr, 1e-4)\n",
    "        a.base_wd = max(a.base_wd, 1e-5)\n",
    "        a.base_b1, a.base_b2 = 0.8, 0.99\n",
    "        print(\"[Preset] convex_flat applied: large batch, low noise, low LR, higher WD\")\n",
    "    elif a.failure_preset == 'nonconvex_sharp':\n",
    "        a.batch_size = min(32, a.batch_size)\n",
    "        a.cytokine_perturb_prob = 0.9\n",
    "        a.cytokine_action_mag = 0.9\n",
    "        a.cytokine_max_spike = max(a.cytokine_max_spike, 10)\n",
    "        a.base_lr = max(a.base_lr, 1e-4)\n",
    "        a.base_wd = 0.0\n",
    "        a.base_b1, a.base_b2 = 0.9, 0.999\n",
    "        print(\"[Preset] nonconvex_sharp applied: small batch, high noise, higher LR, no WD\")\n",
    "\n",
    "    # Step reward logging (optional)\n",
    "    step_log_writer = None\n",
    "    step_log_file = None\n",
    "    if a.log_step_rewards:\n",
    "        step_log_path = \"ppo_multistep_rewards.csv\"\n",
    "        new_file = not os.path.exists(step_log_path)\n",
    "        step_log_file = open(step_log_path, \"a\", newline=\"\")\n",
    "        step_log_writer = csv.writer(step_log_file)\n",
    "        if new_file:\n",
    "            step_log_writer.writerow([\"Setup\",\"Episode\",\"Step\",\"RewardMean\"])\n",
    "\n",
    "    results = []\n",
    "    improvements = []\n",
    "\n",
    "    if a.data_mode == \"peptide\":\n",
    "        # ======== PEPTIDE PIPELINE ========\n",
    "        allele_to_id = load_alleles(a.alleles)\n",
    "        tr = parse_examples(a.train, allele_to_id)\n",
    "        ts = parse_examples(a.test, allele_to_id)\n",
    "        full_train_ds = PeptideDataset(tr, allele_to_id, a.pep_len, a.tcr_len)\n",
    "        N = len(full_train_ds)\n",
    "        idx = list(range(N)); random.shuffle(idx)\n",
    "        v = int(max(1, round(a.val_ratio * N)))\n",
    "        tr_ds = Subset(full_train_ds, idx[v:])\n",
    "        va_ds = Subset(full_train_ds, idx[:v])\n",
    "        te_ds = PeptideDataset(ts, allele_to_id, a.pep_len, a.tcr_len)\n",
    "\n",
    "        if a.oversample:\n",
    "            sampler = build_weighted_sampler(tr_ds, allele_to_id=allele_to_id, by=a.oversample_by, n_bins=a.oversample_bins)\n",
    "            trdl = DataLoader(tr_ds, batch_size=a.batch_size, sampler=sampler, collate_fn=collate_pep)\n",
    "        else:\n",
    "            trdl = DataLoader(tr_ds, batch_size=a.batch_size, shuffle=True, collate_fn=collate_pep)\n",
    "\n",
    "        vadl = DataLoader(va_ds, batch_size=a.batch_size, shuffle=False, collate_fn=collate_pep)\n",
    "        tsdl = DataLoader(te_ds, batch_size=a.batch_size, shuffle=False, collate_fn=collate_pep)\n",
    "\n",
    "        print(f\"\\n=== Training ImmuneNet (fixed {a.immune_act_mode}) ===\")\n",
    "        base_model = ImmuneNet(len(AA_VOCAB), len(allele_to_id),\n",
    "                               act_mode=a.immune_act_mode,\n",
    "                               pep_len=a.pep_len, tcr_len=a.tcr_len).to(d)\n",
    "        train_supervised_seq(base_model, trdl, vadl, d, epochs=a.epochs,\n",
    "                             lr=a.base_lr, weight_decay=a.base_wd, betas=(a.base_b1,a.base_b2))\n",
    "        base_state = deepcopy(base_model.state_dict())\n",
    "\n",
    "        before = eval_loader_metrics_seq(base_model, tsdl, d)\n",
    "        print(\"[Before PPO] \" + \", \".join([f\"{k}={v:.4f}\" for k, v in before.items()]))\n",
    "\n",
    "        ppo_modes = [m.strip() for m in a.ppo_modes.split(\",\") if m.strip()]\n",
    "        for ppo_mode in ppo_modes:\n",
    "            print(f\"\\n=== PPO Activation: {ppo_mode.upper()} (multistep={a.multistep_steps}) ===\")\n",
    "            model = ImmuneNet(len(AA_VOCAB), len(allele_to_id),\n",
    "                              act_mode=a.immune_act_mode,\n",
    "                              pep_len=a.pep_len, tcr_len=a.tcr_len).to(d)\n",
    "            model.load_state_dict(base_state)\n",
    "\n",
    "            # Fresh iterator per mode\n",
    "            def make_state_batch():\n",
    "                if not hasattr(make_state_batch, \"iter\"):\n",
    "                    make_state_batch.iter = iter(trdl)\n",
    "                try:\n",
    "                    pep, tcr, all_idx, cytok_init, y = next(make_state_batch.iter)\n",
    "                except StopIteration:\n",
    "                    make_state_batch.iter = iter(trdl)\n",
    "                    return None\n",
    "                with torch.no_grad():\n",
    "                    z = extract_state_seq(model, pep.to(d), tcr.to(d), all_idx.to(d))\n",
    "                return {\"state\": z, \"cytok_init\": cytok_init, \"pep\": pep, \"tcr\": tcr, \"all\": all_idx}\n",
    "\n",
    "            def env_step_wrapper(batch, action, device, cytok_prev=None):\n",
    "                pep = batch[\"pep\"].to(device); tcr = batch[\"tcr\"].to(device); all_idx = batch[\"all\"].to(device)\n",
    "                return env_step_seq(model, pep, tcr, all_idx, action, device,\n",
    "                                    perturb_prob=a.cytokine_perturb_prob,\n",
    "                                    action_mag=a.cytokine_action_mag,\n",
    "                                    max_spike=a.cytokine_max_spike,\n",
    "                                    cytok_prev=cytok_prev)\n",
    "\n",
    "            setup_name = f\"Immune({a.immune_act_mode})+PPO({ppo_mode})\"\n",
    "            ppo_train_multistep(\n",
    "                state_dim=256,\n",
    "                make_state_batch=make_state_batch,\n",
    "                env_step_fn=env_step_wrapper,\n",
    "                policy_act_mode=ppo_mode,\n",
    "                switch_epoch=a.ppo_switch_epoch,\n",
    "                ppo_epochs=a.ppo_epochs, episodes=a.ppo_episodes,\n",
    "                discount_gamma=a.discount_gamma, multistep_steps=a.multistep_steps,\n",
    "                log_step_rewards=a.log_step_rewards, step_log_writer=step_log_writer, setup_name=setup_name\n",
    "            )\n",
    "            after = eval_loader_metrics_seq(model, tsdl, d)\n",
    "            print(\"[After  PPO] \" + \", \".join([f\"{k}={v:.4f}\" for k, v in after.items()]))\n",
    "            results.append([setup_name, after[\"MSE\"], after[\"RMSE\"], after[\"MAE\"], after[\"R2\"], after[\"Pearson\"]])\n",
    "\n",
    "            # --- Improvement tracking ---\n",
    "            imp = {\n",
    "                \"Setup\": setup_name,\n",
    "                \"ΔMSE\": before[\"MSE\"] - after[\"MSE\"],\n",
    "                \"ΔR2\":  after[\"R2\"] - before[\"R2\"],\n",
    "                \"ΔPearson\": after[\"Pearson\"] - before[\"Pearson\"]\n",
    "            }\n",
    "            improvements.append(imp)\n",
    "            print(f\"[Improvement] {setup_name}: ΔMSE={imp['ΔMSE']:.4f}, ΔR2={imp['ΔR2']:.4f}, ΔPearson={imp['ΔPearson']:.4f}\")\n",
    "\n",
    "    else:\n",
    "        # ======== PBMC PIPELINE ========\n",
    "        if not HAS_MUON:\n",
    "            print(\"❌ --data_mode pbmc requires muon + mudatasets installed.\")\n",
    "            return\n",
    "        fused_train, y_train, fused_test, y_test = pbmc_build_embeddings(sample_size=a.pbmc_sample, seed=1)\n",
    "\n",
    "        # build datasets\n",
    "        tr_ds = VectorDataset(fused_train, y_train)\n",
    "        te_ds = VectorDataset(fused_test, y_test)\n",
    "        idx = np.arange(len(tr_ds)); np.random.shuffle(idx)\n",
    "        v = int(max(1, round(a.val_ratio * len(idx))))\n",
    "        va_idx, tr_idx = idx[:v], idx[v:]\n",
    "        va_ds = torch.utils.data.Subset(tr_ds, va_idx)\n",
    "        tr_sub = torch.utils.data.Subset(tr_ds, tr_idx)\n",
    "\n",
    "        trdl = DataLoader(tr_sub, batch_size=a.batch_size, shuffle=True,  collate_fn=collate_vec)\n",
    "        vadl = DataLoader(va_ds,  batch_size=a.batch_size, shuffle=False, collate_fn=collate_vec)\n",
    "        tsdl = DataLoader(te_ds,  batch_size=a.batch_size, shuffle=False, collate_fn=collate_vec)\n",
    "\n",
    "        in_dim = fused_train.shape[1]\n",
    "        print(f\"\\n=== Training VectorNet (fixed {a.immune_act_mode}) on PBMC fused dim={in_dim} ===\")\n",
    "        base_model = VectorNet(in_dim=in_dim, act_mode=a.immune_act_mode).to(d)\n",
    "        train_supervised_vec(base_model, trdl, vadl, d, epochs=a.epochs,\n",
    "                             lr=a.base_lr, weight_decay=a.base_wd, betas=(a.base_b1,a.base_b2))\n",
    "        base_state = deepcopy(base_model.state_dict())\n",
    "\n",
    "        before = eval_loader_metrics_vec(base_model, tsdl, d)\n",
    "        print(\"[Before PPO] \" + \", \".join([f\"{k}={v:.4f}\" for k, v in before.items()]))\n",
    "\n",
    "        def env_vec(batch, action, device, cytok_prev=None):\n",
    "            X = batch[\"X\"].to(device)\n",
    "            return env_step_vec(model, X, action, device,\n",
    "                                perturb_prob=a.cytokine_perturb_prob,\n",
    "                                action_mag=a.cytokine_action_mag,\n",
    "                                max_spike=a.cytokine_max_spike,\n",
    "                                cytok_prev=cytok_prev)\n",
    "\n",
    "        ppo_modes = [m.strip() for m in a.ppo_modes.split(\",\") if m.strip()]\n",
    "        for ppo_mode in ppo_modes:\n",
    "            print(f\"\\n=== PPO Activation: {ppo_mode.upper()} (multistep={a.multistep_steps}) ===\")\n",
    "            model = VectorNet(in_dim=in_dim, act_mode=a.immune_act_mode).to(d)\n",
    "            model.load_state_dict(base_state)\n",
    "\n",
    "            def make_state_batch():\n",
    "                if not hasattr(make_state_batch, \"iter\"):\n",
    "                    make_state_batch.iter = iter(trdl)\n",
    "                try:\n",
    "                    X, cytok_init, y = next(make_state_batch.iter)\n",
    "                except StopIteration:\n",
    "                    make_state_batch.iter = iter(trdl)\n",
    "                    return None\n",
    "                with torch.no_grad():\n",
    "                    z = extract_state_vec(model, X.to(d))\n",
    "                return {\"state\": z, \"cytok_init\": cytok_init, \"X\": X}\n",
    "\n",
    "            def env_step_wrapper(batch, action, device, cytok_prev=None):\n",
    "                return env_vec(batch, action, device, cytok_prev)\n",
    "\n",
    "            setup_name = f\"Vector({a.immune_act_mode})+PPO({ppo_mode})\"\n",
    "            ppo_train_multistep(\n",
    "                state_dim=256,\n",
    "                make_state_batch=make_state_batch,\n",
    "                env_step_fn=env_step_wrapper,\n",
    "                policy_act_mode=ppo_mode,\n",
    "                switch_epoch=a.ppo_switch_epoch,\n",
    "                ppo_epochs=a.ppo_epochs, episodes=a.ppo_episodes,\n",
    "                discount_gamma=a.discount_gamma, multistep_steps=a.multistep_steps,\n",
    "                log_step_rewards=a.log_step_rewards, step_log_writer=step_log_writer, setup_name=setup_name\n",
    "            )\n",
    "            after = eval_loader_metrics_vec(model, tsdl, d)\n",
    "            print(\"[After  PPO] \" + \", \".join([f\"{k}={v:.4f}\" for k, v in after.items()]))\n",
    "            results.append([setup_name, after[\"MSE\"], after[\"RMSE\"], after[\"MAE\"], after[\"R2\"], after[\"Pearson\"]])\n",
    "\n",
    "            imp = {\n",
    "                \"Setup\": setup_name,\n",
    "                \"ΔMSE\": before[\"MSE\"] - after[\"MSE\"],\n",
    "                \"ΔR2\":  after[\"R2\"] - before[\"R2\"],\n",
    "                \"ΔPearson\": after[\"Pearson\"] - before[\"Pearson\"]\n",
    "            }\n",
    "            improvements.append(imp)\n",
    "            print(f\"[Improvement] {setup_name}: ΔMSE={imp['ΔMSE']:.4f}, ΔR2={imp['ΔR2']:.4f}, ΔPearson={imp['ΔPearson']:.4f}\")\n",
    "\n",
    "    # --- Save results ---\n",
    "    with open(\"gated_relu_multistep.csv\", \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Setup\", \"MSE\", \"RMSE\", \"MAE\", \"R2\", \"Pearson\"])\n",
    "        writer.writerows(results)\n",
    "    print(\"\\n✅ Results saved to gated_relu_multistep.csv\")\n",
    "\n",
    "    with open(\"performance_improvements.csv\", \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Setup\", \"ΔMSE\", \"ΔR2\", \"ΔPearson\"])\n",
    "        for imp in improvements:\n",
    "            writer.writerow([imp[\"Setup\"], imp[\"ΔMSE\"], imp[\"ΔR2\"], imp[\"ΔPearson\"]])\n",
    "    print(\"📈 Improvement summary saved to performance_improvements.csv\")\n",
    "\n",
    "    # --- Optional plot (if matplotlib present) ---\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import pandas as pd\n",
    "        if os.path.exists(\"performance_improvements.csv\"):\n",
    "            df = pd.read_csv(\"performance_improvements.csv\")\n",
    "            plt.figure(figsize=(8,4))\n",
    "            for metric in [\"ΔMSE\", \"ΔR2\", \"ΔPearson\"]:\n",
    "                plt.plot(df[\"Setup\"], df[metric], marker='o', label=metric)\n",
    "            plt.xticks(rotation=30, ha='right')\n",
    "            plt.ylabel(\"Improvement (After - Before)\")\n",
    "            plt.title(\"Performance Improvement per Method\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"performance_improvement_plot.png\", dpi=160)\n",
    "            print(\"📊 Saved performance plot: performance_improvement_plot.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"(Plotting skipped) {e}\")\n",
    "\n",
    "    # --- Best setup by ΔR2 ---\n",
    "    if len(improvements) > 0:\n",
    "        best = max(improvements, key=lambda x: x[\"ΔR2\"])\n",
    "        print(f\"🏆 Best R2 improvement: {best['Setup']} (ΔR2={best['ΔR2']:.4f})\")\n",
    "\n",
    "    # Close step log file if used\n",
    "    try:\n",
    "        if step_log_writer is not None:\n",
    "            step_log_file.close()\n",
    "            print(\"✅ Per-step curves saved to ppo_multistep_rewards.csv\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except SystemExit:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b0dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbbb185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
