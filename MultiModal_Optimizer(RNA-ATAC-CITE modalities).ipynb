{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43f1715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import muon as mu\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a0b4128b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ File filtered_feature_bc_matrix.h5 from pbmc10k_multiome has been found at /Users/boabangfrancis/mudatasets/pbmc10k_multiome/filtered_feature_bc_matrix.h5\n",
      "■ Checksum is validated (md5) for filtered_feature_bc_matrix.h5\n",
      "■ File atac_fragments.tsv.gz from pbmc10k_multiome has been found at /Users/boabangfrancis/mudatasets/pbmc10k_multiome/atac_fragments.tsv.gz\n",
      "■ Checksum is validated (md5) for atac_fragments.tsv.gz\n",
      "■ File atac_fragments.tsv.gz.tbi from pbmc10k_multiome has been found at /Users/boabangfrancis/mudatasets/pbmc10k_multiome/atac_fragments.tsv.gz.tbi\n",
      "■ Checksum is validated (md5) for atac_fragments.tsv.gz.tbi\n",
      "■ File atac_peaks.bed from pbmc10k_multiome has been found at /Users/boabangfrancis/mudatasets/pbmc10k_multiome/atac_peaks.bed\n",
      "■ Checksum is validated (md5) for atac_peaks.bed\n",
      "■ File atac_peak_annotation.tsv from pbmc10k_multiome has been found at /Users/boabangfrancis/mudatasets/pbmc10k_multiome/atac_peak_annotation.tsv\n",
      "■ Checksum is validated (md5) for atac_peak_annotation.tsv\n",
      "■ Loading filtered_feature_bc_matrix.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/mudatasets/core.py:203: UserWarning: Dataset is in the 10X .h5 format and can't be loaded as backed.\n",
      "  warn(\"Dataset is in the 10X .h5 format and can't be loaded as backed.\")\n",
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/anndata/_core/anndata.py:1776: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added `interval` annotation for features from /Users/boabangfrancis/mudatasets/pbmc10k_multiome/filtered_feature_bc_matrix.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/anndata/_core/anndata.py:1776: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/mudata/_core/mudata.py:1598: FutureWarning: From 0.4 .update() will not pull obs/var columns from individual modalities by default anymore. Set mudata.set_options(pull_on_update=False) to adopt the new behaviour, which will become the default. Use new pull_obs/pull_var and push_obs/push_var methods for more flexibility.\n",
      "  self._update_attr(\"var\", axis=0, join_common=join_common)\n",
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/mudata/_core/mudata.py:947: UserWarning: var_names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  warnings.warn(\n",
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/mudata/_core/mudata.py:1461: FutureWarning: From 0.4 .update() will not pull obs/var columns from individual modalities by default anymore. Set mudata.set_options(pull_on_update=False) to adopt the new behaviour, which will become the default. Use new pull_obs/pull_var and push_obs/push_var methods for more flexibility.\n",
      "  self._update_attr(\"obs\", axis=1, join_common=join_common)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added peak annotation from /Users/boabangfrancis/mudatasets/pbmc10k_multiome/atac_peak_annotation.tsv to .uns['atac']['peak_annotation']\n",
      "Added gene names to peak annotation in .uns['atac']['peak_annotation']\n",
      "Located fragments file: /Users/boabangfrancis/mudatasets/pbmc10k_multiome/atac_fragments.tsv.gz\n",
      "pysam is not available. It is required to work with the fragments file.                 Install pysam from PyPI (`pip install pysam`)                 or from GitHub (`pip install git+https://github.com/pysam-developers/pysam`)\n",
      "Available modalities: ['rna', 'atac']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:82: UserWarning: `flavor='seurat_v3'` expects raw count data, but non-integers were found.\n",
      "  return fn(*args_all, **kw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common cells: 11852\n",
      "RNA - Train shape: (9481, 50), Test shape: (2371, 50)\n",
      "ATAC - Train shape: (9481, 50), Test shape: (2371, 50)\n",
      "Modality splits: {'rna': (0, 50), 'atac': (50, 100)}\n",
      "\n",
      "--- Training adam on modality=rna ---\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/Users/boabangfrancis/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2662 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 2031       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 2          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33505931 |\n",
      "|    clip_fraction        | 0.717      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -71        |\n",
      "|    explained_variance   | -0.0511    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.22       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0969    |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 2.51       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 1890     |\n",
      "|    iterations           | 3        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 6144     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.338546 |\n",
      "|    clip_fraction        | 0.714    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -71      |\n",
      "|    explained_variance   | -0.0193  |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 0.225    |\n",
      "|    n_updates            | 20       |\n",
      "|    policy_gradient_loss | -0.0828  |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 11.1     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1809      |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4055072 |\n",
      "|    clip_fraction        | 0.72      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -71       |\n",
      "|    explained_variance   | 0.04      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.484     |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | -0.0853   |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 12.3      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 814\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trained_models, results_reward\n\u001b[1;32m    813\u001b[0m adaptive_thresholds \u001b[38;5;241m=\u001b[39m PerGeneAdaptiveThreshold(modality_dims)\n\u001b[0;32m--> 814\u001b[0m trained_models, rewards \u001b[38;5;241m=\u001b[39m train_and_evaluate_optimizers(\n\u001b[1;32m    815\u001b[0m     modality_choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrna\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# or \"all\", \"atac\", \"adt\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m     expression_train\u001b[38;5;241m=\u001b[39mfused_train,\n\u001b[1;32m    817\u001b[0m     pseudotime_train\u001b[38;5;241m=\u001b[39mpseudotime_train,\n\u001b[1;32m    818\u001b[0m     expression_test\u001b[38;5;241m=\u001b[39mfused_test,\n\u001b[1;32m    819\u001b[0m     pseudotime_test\u001b[38;5;241m=\u001b[39mpseudotime_test,\n\u001b[1;32m    820\u001b[0m     modality_splits\u001b[38;5;241m=\u001b[39mmodality_splits,\n\u001b[1;32m    821\u001b[0m     gene_names\u001b[38;5;241m=\u001b[39mselected_gene_names,\n\u001b[1;32m    822\u001b[0m     adaptive_thresholds\u001b[38;5;241m=\u001b[39madaptive_thresholds,\n\u001b[1;32m    823\u001b[0m     optimizers_to_run\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    824\u001b[0m     train_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m,\n\u001b[1;32m    825\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    826\u001b[0m )\n",
      "Cell \u001b[0;32mIn[66], line 747\u001b[0m, in \u001b[0;36mtrain_and_evaluate_optimizers\u001b[0;34m(modality_choice, expression_train, pseudotime_train, expression_test, pseudotime_test, modality_splits, gene_names, adaptive_thresholds, optimizers_to_run, train_steps, save_dir)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on modality=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodality_choice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    746\u001b[0m model \u001b[38;5;241m=\u001b[39m PPOClass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, seed\u001b[38;5;241m=\u001b[39mSEED, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbase_kwargs)\n\u001b[0;32m--> 747\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mtrain_steps)\n\u001b[1;32m    748\u001b[0m trained_models[opt_name] \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    750\u001b[0m \u001b[38;5;66;03m# Evaluate mean reward on test environment\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m    316\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[1;32m    317\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    318\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[1;32m    319\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[1;32m    320\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[1;32m    321\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m    322\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_rollouts(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, callback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer, n_rollout_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps)\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(clipped_actions)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mstep(\n\u001b[1;32m     59\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shimmy/openai_gym_compatibility.py:250\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgym_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[0;32mIn[66], line 305\u001b[0m, in \u001b[0;36mGRNEnvWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 305\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m:  \u001b[38;5;66;03m# Gym >=0.26\u001b[39;00m\n\u001b[1;32m    307\u001b[0m         obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n",
      "Cell \u001b[0;32mIn[66], line 252\u001b[0m, in \u001b[0;36mPBMC_CRISPR_MultiModalEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction length mismatch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, delta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(action):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[i] \u001b[38;5;241m+\u001b[39m delta \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_magnitude, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5.0\u001b[39m, \u001b[38;5;241m5.0\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperturb_prob:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_crispr_perturbation()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2096\u001b[0m, in \u001b[0;36m_clip_dispatcher\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;124;03m    Return selected slices of an array along given axis.\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2091\u001b[0m \n\u001b[1;32m   2092\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompress\u001b[39m\u001b[38;5;124m'\u001b[39m, condition, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m-> 2096\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clip_dispatcher\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, a_min, a_max)\n\u001b[1;32m   2100\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Full updated notebook cell: PBMC Multi-modal PPO pipeline\n",
    "# Copy entire cell into your notebook and run.\n",
    "\n",
    "# -----------------------------\n",
    "# Imports & settings\n",
    "# -----------------------------\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "import muon as mu\n",
    "import mudatasets as mds\n",
    "import scanpy as sc\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility & device\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config\n",
    "MAX_STEPS = 100\n",
    "PERTURB_PROB = 0.5\n",
    "MAX_PERTURB = 40\n",
    "N_EVAL_EPISODES = 30\n",
    "OUT_DIR = \"pbmc_multi_output\"\n",
    "PLOTS_DIR = os.path.join(OUT_DIR, \"pseudotime_plots\")\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1..4: Load & preprocess PBMC multiome\n",
    "# -----------------------------\n",
    "mdata = mds.load(\"pbmc10k_multiome\", full=True)\n",
    "mdata.var_names_make_unique()\n",
    "print(\"Available modalities:\", list(mdata.mod.keys()))\n",
    "\n",
    "# RNA preprocessing\n",
    "rna = mdata.mod['rna']\n",
    "sc.pp.filter_cells(rna, min_genes=200)\n",
    "sc.pp.filter_genes(rna, min_cells=3)\n",
    "sc.pp.normalize_total(rna, target_sum=1e4)\n",
    "sc.pp.log1p(rna)\n",
    "sc.pp.highly_variable_genes(rna, n_top_genes=2000, subset=True)\n",
    "sc.tl.pca(rna, n_comps=50)\n",
    "\n",
    "# ATAC preprocessing\n",
    "atac = mdata.mod['atac']\n",
    "sc.pp.normalize_total(atac, target_sum=1e4)\n",
    "sc.pp.log1p(atac)\n",
    "sc.pp.highly_variable_genes(atac, n_top_genes=5000, flavor=\"seurat_v3\", subset=True)\n",
    "sc.tl.pca(atac, n_comps=50)\n",
    "\n",
    "# ADT (optional)\n",
    "adt = mdata.mod['adt'] if 'adt' in mdata.mod else None\n",
    "if adt is not None:\n",
    "    adt.X = np.log1p(adt.X)\n",
    "    sc.pp.highly_variable_genes(adt, n_top_genes=50, subset=True)\n",
    "    sc.tl.pca(adt, n_comps=20)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Align cells across modalities & build modality_data dict\n",
    "# -----------------------------\n",
    "common_cells = rna.obs_names.intersection(atac.obs_names)\n",
    "if adt is not None:\n",
    "    common_cells = common_cells.intersection(adt.obs_names)\n",
    "common_cells = np.array(common_cells)\n",
    "print(\"Number of common cells:\", len(common_cells))\n",
    "\n",
    "# Use the PCA embeddings computed earlier\n",
    "z_rna = rna[common_cells].obsm['X_pca']\n",
    "z_atac = atac[common_cells].obsm['X_pca']\n",
    "if adt is not None:\n",
    "    z_adt = adt[common_cells].obsm['X_pca']\n",
    "    modality_data = {'rna': z_rna, 'atac': z_atac, 'adt': z_adt}\n",
    "else:\n",
    "    modality_data = {'rna': z_rna, 'atac': z_atac}\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Train/test split per modality & scaling\n",
    "# -----------------------------\n",
    "expression_train = {}\n",
    "expression_test = {}\n",
    "pseudotime_train = {}\n",
    "pseudotime_test = {}\n",
    "scalers = {}\n",
    "\n",
    "for mod, data in modality_data.items():\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    scalers[mod] = scaler\n",
    "\n",
    "    train_idx, test_idx = train_test_split(np.arange(data_scaled.shape[0]), test_size=0.2, random_state=SEED)\n",
    "    expression_train[mod] = data_scaled[train_idx]\n",
    "    expression_test[mod] = data_scaled[test_idx]\n",
    "\n",
    "    pseudotime = np.arange(data_scaled.shape[0], dtype=np.float32)\n",
    "    pseudotime_train[mod] = pseudotime[train_idx]\n",
    "    pseudotime_test[mod] = pseudotime[test_idx]\n",
    "\n",
    "    print(f\"{mod.upper()} - Train shape: {expression_train[mod].shape}, Test shape: {expression_test[mod].shape}\")\n",
    "\n",
    "# fused embeddings (concatenate modalities order-preserving)\n",
    "fused_train = np.concatenate([expression_train[m] for m in expression_train.keys()], axis=1)\n",
    "fused_test = np.concatenate([expression_test[m] for m in expression_test.keys()], axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 7: Modality splits / gene names\n",
    "# -----------------------------\n",
    "modality_dims = {mod: expression_train[mod].shape[1] for mod in expression_train.keys()}\n",
    "# Build modality_splits dict: modality -> (start_idx, end_idx) on fused matrices\n",
    "starts = np.cumsum([0] + list(modality_dims.values()))[:-1]\n",
    "modality_splits = {}\n",
    "idx = 0\n",
    "for mod, dim in modality_dims.items():\n",
    "    start = idx\n",
    "    end = idx + dim\n",
    "    modality_splits[mod] = (start, end)\n",
    "    idx = end\n",
    "\n",
    "# Make selected_gene_names: simple list of generic names length == fused features\n",
    "total_features = fused_train.shape[1]\n",
    "selected_gene_names = [f\"feat_{i}\" for i in range(total_features)]\n",
    "\n",
    "print(\"Modality splits:\", modality_splits)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 8: Adaptive Threshold per modality\n",
    "# -----------------------------\n",
    "class PerGeneAdaptiveThreshold:\n",
    "    def __init__(self, modality_dims, alpha=0.1):\n",
    "        self.thresholds = {mod: {i: 0.0 for i in range(dim)} for mod, dim in modality_dims.items()}\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update(self, gene_rewards):\n",
    "        for mod, rewards in gene_rewards.items():\n",
    "            for gene_id, reward in rewards.items():\n",
    "                if reward is None or (isinstance(reward, float) and np.isnan(reward)):\n",
    "                    continue\n",
    "                self.thresholds[mod][gene_id] = self.alpha * float(reward) + (1 - self.alpha) * self.thresholds[mod].get(gene_id, 0.0)\n",
    "\n",
    "    def get(self, mod, gene_id):\n",
    "        return float(self.thresholds.get(mod, {}).get(gene_id, 0.0))\n",
    "\n",
    "adaptive_thresholds = PerGeneAdaptiveThreshold(modality_dims)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 9: Multi-modal PBMC CRISPR environment\n",
    "# -----------------------------\n",
    "class PBMC_CRISPR_MultiModalEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, expression_dict_or_matrix, pseudotime_dict_or_array, modality_splits,\n",
    "                 max_steps=MAX_STEPS, adaptive_thresholds=None, device='cpu',\n",
    "                 action_magnitude=0.25, perturb_prob=0.1, max_perturb=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Accept either dicts (per-modality) or already-fused matrices\n",
    "        if isinstance(expression_dict_or_matrix, dict):\n",
    "            # build fused matrix in modality_splits order\n",
    "            self.expression_dict = {mod: np.asarray(expression_dict_or_matrix[mod], dtype=np.float32) for mod in modality_splits.keys()}\n",
    "            self.expression = np.concatenate([self.expression_dict[mod] for mod in modality_splits.keys()], axis=1)\n",
    "            # pseudotime: use first modality's pseudotime (cell-level)\n",
    "            self.pseudotime = np.asarray(next(iter(pseudotime_dict_or_array.values())), dtype=np.float32)\n",
    "        else:\n",
    "            self.expression = np.asarray(expression_dict_or_matrix, dtype=np.float32)\n",
    "            self.expression_dict = {}\n",
    "            start = 0\n",
    "            for mod, (s, e) in modality_splits.items():\n",
    "                self.expression_dict[mod] = self.expression[:, s:e]\n",
    "            self.pseudotime = np.asarray(pseudotime_dict_or_array, dtype=np.float32)\n",
    "\n",
    "        self.modality_splits = modality_splits\n",
    "        self.modality_dims = {m: (e - s) for m, (s, e) in modality_splits.items()}\n",
    "        self.n_cells, self.n_genes = self.expression.shape\n",
    "        self.max_steps = max_steps\n",
    "        self.adaptive_thresholds = adaptive_thresholds\n",
    "        self.device = device\n",
    "        self.action_magnitude = action_magnitude\n",
    "        self.perturb_prob = perturb_prob\n",
    "        self.max_perturb = max_perturb\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self.n_genes,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_genes,), dtype=np.float32)\n",
    "\n",
    "        # bookkeeping\n",
    "        self.current_cell = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.idx = np.random.randint(self.n_cells)\n",
    "        self.state = self.expression[self.idx].copy()\n",
    "        self.original_state = self.state.copy()\n",
    "\n",
    "        eligible_idxs = np.where(np.arange(self.n_cells) > self.idx)[0]\n",
    "        if len(eligible_idxs) == 0:\n",
    "            eligible_idxs = np.array([self.idx])\n",
    "        target_idx = np.random.choice(eligible_idxs)\n",
    "        self.target = self.expression[target_idx].copy()\n",
    "\n",
    "        self.steps = 0\n",
    "        self.history = [self.state.copy()]\n",
    "        self.knockout_genes = set()\n",
    "        self.overexpressed_genes = set()\n",
    "        self._apply_crispr_perturbation()\n",
    "        self.current_cell = 0\n",
    "        return self.state.copy()\n",
    "\n",
    "    def _apply_crispr_perturbation(self):\n",
    "        n_perturb = np.random.randint(1, self.max_perturb + 1)\n",
    "        for _ in range(n_perturb):\n",
    "            gene = np.random.randint(0, self.n_genes)\n",
    "            if np.random.rand() < 0.5:\n",
    "                self.state[gene] = 0.0\n",
    "                self.knockout_genes.add(int(gene))\n",
    "            else:\n",
    "                self.state[gene] = self.state[gene] * 2.0\n",
    "                self.overexpressed_genes.add(int(gene))\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.asarray(action, dtype=np.float32).ravel()\n",
    "        if action.shape[0] != self.n_genes:\n",
    "            raise ValueError(\"Action length mismatch.\")\n",
    "        for i, delta in enumerate(action):\n",
    "            self.state[i] = np.clip(self.state[i] + delta * self.action_magnitude, -5.0, 5.0)\n",
    "\n",
    "        if np.random.rand() < self.perturb_prob:\n",
    "            self._apply_crispr_perturbation()\n",
    "\n",
    "        old_mse = float(np.mean((self.history[-1] - self.target) ** 2))\n",
    "        new_mse = float(np.mean((self.state - self.target) ** 2))\n",
    "        reward = old_mse - new_mse\n",
    "\n",
    "        # subtract adaptive thresholds per modality (if provided)\n",
    "        if self.adaptive_thresholds is not None:\n",
    "            for mod, (start, end) in self.modality_splits.items():\n",
    "                for local_idx, g in enumerate(range(start, end)):\n",
    "                    reward -= self.adaptive_thresholds.get(mod, local_idx)\n",
    "\n",
    "        self.steps += 1\n",
    "        self.history.append(self.state.copy())\n",
    "        terminated = self.steps >= self.max_steps\n",
    "        done = terminated\n",
    "        info = {}\n",
    "        self.current_cell += 1\n",
    "        return self.state.copy(), float(reward), done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step {self.steps} - state (first 10): {self.state[:10]}\")\n",
    "        print(f\"Knockouts: {sorted(list(self.knockout_genes))[:10]}, Overexpr: {sorted(list(self.overexpressed_genes))[:10]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# SB3 Env wrapper\n",
    "# -----------------------------\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class GRNEnvWrapper(gym.Env):\n",
    "    def __init__(self, base_env):\n",
    "        super().__init__()\n",
    "        self.env = base_env\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.env.n_genes,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        result = self.env.reset(**kwargs)\n",
    "        # Handle Gym >=0.26 returning (obs, info)\n",
    "        if isinstance(result, tuple) and len(result) == 2:\n",
    "            obs, info = result\n",
    "            return np.asarray(obs, dtype=np.float32), info\n",
    "        else:\n",
    "            return np.asarray(result, dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        result = self.env.step(action)\n",
    "        if len(result) == 5:  # Gym >=0.26\n",
    "            obs, reward, terminated, truncated, info = result\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            obs, reward, done, info = result\n",
    "        return np.asarray(obs, dtype=np.float32), reward, done, info\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        # Optional: delegate seeding to the base environment\n",
    "        if hasattr(self.env, 'seed'):\n",
    "            return self.env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        return [seed]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# make_env_factory (multi-modality aware)\n",
    "# -----------------------------\n",
    "def make_env_factory(expression, pseudotime, modality_splits, adaptive_thresholds,\n",
    "                     perturb_prob=PERTURB_PROB, max_perturb=MAX_PERTURB):\n",
    "    def _init():\n",
    "        base_env = PBMC_CRISPR_MultiModalEnv(\n",
    "            expression_dict_or_matrix=expression,\n",
    "            pseudotime_dict_or_array=pseudotime,\n",
    "            modality_splits=modality_splits,\n",
    "            max_steps=MAX_STEPS,\n",
    "            adaptive_thresholds=adaptive_thresholds,\n",
    "            device=DEVICE,\n",
    "            action_magnitude=0.25,\n",
    "            perturb_prob=perturb_prob,\n",
    "            max_perturb=max_perturb\n",
    "        )\n",
    "        return GRNEnvWrapper(base_env)\n",
    "    return _init\n",
    "\n",
    "# -----------------------------\n",
    "# evaluate_and_plot_multi_modality\n",
    "# -----------------------------\n",
    "def evaluate_and_plot_multi_modality(model, algo_name, expression_test, pseudotime_test,\n",
    "                                     gene_names, modality_splits, adaptive_thresholds,\n",
    "                                     n_episodes=50, save_dir=PLOTS_DIR):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    results = []\n",
    "    eval_env_factory = make_env_factory(expression_test, pseudotime_test, modality_splits, adaptive_thresholds)\n",
    "\n",
    "    for mod_name, (start_idx, end_idx) in modality_splits.items():\n",
    "        print(f\"Evaluating modality: {mod_name}\")\n",
    "        for gene_idx in range(start_idx, end_idx):\n",
    "            gene_name = gene_names[gene_idx] if gene_idx < len(gene_names) else f\"g{gene_idx}\"\n",
    "            y_true, y_pred = [], []\n",
    "            perturbed_vals, original_vals, pseudotimes = [], [], []\n",
    "\n",
    "            for ep in range(n_episodes):\n",
    "                env = eval_env_factory()\n",
    "                try:\n",
    "                    obs = env.reset()\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                original = env.env.original_state.copy()\n",
    "                target = env.env.target.copy()\n",
    "                pt_idx = getattr(env.env, \"current_cell\", 0)\n",
    "                pt_value = float(env.env.pseudotime[pt_idx]) if len(env.env.pseudotime) > 0 else 0.0\n",
    "\n",
    "                done = False\n",
    "                traj_pred = []\n",
    "\n",
    "                while not done:\n",
    "                    try:\n",
    "                        action, _ = model.predict(obs, deterministic=True)\n",
    "                        step_result = env.step(action)\n",
    "                        if len(step_result) == 5:\n",
    "                            obs, reward, terminated, truncated, info = step_result\n",
    "                            done = terminated or truncated\n",
    "                        else:\n",
    "                            obs, reward, done, info = step_result\n",
    "                    except Exception:\n",
    "                        break\n",
    "                    traj_pred.append(env.env.state[gene_idx])\n",
    "\n",
    "                if len(traj_pred) == 0:\n",
    "                    continue\n",
    "\n",
    "                final_state = env.env.history[-1]\n",
    "                delta = float(final_state[gene_idx] - original[gene_idx])\n",
    "                label = 1 if target[gene_idx] > original[gene_idx] else 0\n",
    "                prediction = 1 if delta > 0 else 0\n",
    "\n",
    "                y_true.append(label)\n",
    "                y_pred.append(prediction)\n",
    "                perturbed_vals.append(float(final_state[gene_idx]))\n",
    "                original_vals.append(float(original[gene_idx]))\n",
    "                pseudotimes.append(pt_value)\n",
    "\n",
    "            if len(y_true) == 0:\n",
    "                continue\n",
    "\n",
    "            # metrics\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "            rec = recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "            try:\n",
    "                auprc = average_precision_score(y_true, y_pred)\n",
    "            except Exception:\n",
    "                auprc = np.nan\n",
    "\n",
    "            mse = mean_squared_error(original_vals, perturbed_vals)\n",
    "            rmse = math.sqrt(mse)\n",
    "            mae = mean_absolute_error(original_vals, perturbed_vals)\n",
    "            r2 = r2_score(original_vals, perturbed_vals)\n",
    "            pc = np.corrcoef(original_vals, perturbed_vals)[0, 1] if np.std(original_vals) != 0 else 0.0\n",
    "\n",
    "            results.append({\n",
    "                \"Algorithm\": algo_name,\n",
    "                \"Modality\": mod_name,\n",
    "                \"Gene\": gene_name,\n",
    "                \"Accuracy\": acc,\n",
    "                \"Precision\": prec,\n",
    "                \"Recall\": rec,\n",
    "                \"F1\": f1,\n",
    "                \"AUPRC\": auprc,\n",
    "                \"Final Expression MSE\": mse,\n",
    "                \"Final Expression RMSE\": rmse,\n",
    "                \"Final Expression MAE\": mae,\n",
    "                \"Final Expression R²\": r2,\n",
    "                \"Final Expression PearsonCorr\": pc\n",
    "            })\n",
    "\n",
    "            # plot pseudotime\n",
    "            try:\n",
    "                df = pd.DataFrame({\n",
    "                    \"pseudotime\": pseudotimes,\n",
    "                    \"original_expression\": original_vals,\n",
    "                    \"perturbed_expression\": perturbed_vals\n",
    "                })\n",
    "                df['delta'] = df['perturbed_expression'] - df['original_expression']\n",
    "                df['label'] = df['delta'].apply(lambda x: \"Up\" if x > 0 else \"Down\")\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                sns.scatterplot(data=df, x=\"pseudotime\", y=\"perturbed_expression\", hue=\"label\", style=\"label\")\n",
    "                sns.lineplot(data=df.sort_values('pseudotime'), x=\"pseudotime\", y=\"perturbed_expression\", lw=1, alpha=0.5)\n",
    "                plt.title(f\"{algo_name} — {mod_name} — {gene_name} Perturbation\")\n",
    "                plt.xlabel(\"Pseudotime\")\n",
    "                plt.ylabel(\"Expression (z-score)\")\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(save_dir, f\"{algo_name}_{mod_name}_{gene_name}.png\"), dpi=300)\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(\"Plot error:\", e)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# -----------------------------\n",
    "# Custom optimizers (Padam, ASGDAdam, ASGDAmsgrad)\n",
    "# -----------------------------\n",
    "# Implementations included so we can use them as optimizer_class via policy_kwargs in PPO wrappers.\n",
    "# (Identical implementations as earlier; kept minimal here to register classes.)\n",
    "\n",
    "def count_nonzero(tensor):\n",
    "    return int((tensor != 0).sum().item())\n",
    "\n",
    "class ASGDAdam(Optimizer):\n",
    "    \"\"\"ASGD-style optimizer using separate lr_min/lr_max per step.\"\"\"\n",
    "    def __init__(self, params, lr=None, beta1=0.9, beta2=0.999, eps=1e-8,\n",
    "                 lr_min=1e-4, lr_max=3e-4):\n",
    "        defaults = dict(beta1=beta1, beta2=beta2, eps=eps, lr_min=lr_min, lr_max=lr_max)\n",
    "        super().__init__(params, defaults)\n",
    "        self.last_total_nonzero_fmin = 0\n",
    "        self.last_total_nonzero_fmax = 0\n",
    "        self.last_lr = lr_max\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        total_nonzero_fmin, total_nonzero_fmax = 0, 0\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2, eps = group['beta1'], group['beta2'], group['eps']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                g = p.grad\n",
    "                state = self.state[p]\n",
    "                if not state:\n",
    "                    state['t'] = 0\n",
    "                    state['m'] = torch.zeros_like(p)\n",
    "                    state['v'] = torch.zeros_like(p)\n",
    "                    state['v_prev'] = torch.zeros_like(p)\n",
    "                m, v = state['m'], state['v']\n",
    "                state['t'] += 1\n",
    "                t = state['t']\n",
    "                m.mul_(beta1).add_(g, alpha=1-beta1)\n",
    "                v.mul_(beta2).addcmul_(g, g, value=1-beta2)\n",
    "                dv = v - state['v_prev']\n",
    "                state['v_prev'].copy_(v)\n",
    "                f_min = (dv > 0).to(dtype=torch.int32)\n",
    "                f_max = (dv <= 0).to(dtype=torch.int32)\n",
    "                total_nonzero_fmin += count_nonzero(f_min)\n",
    "                total_nonzero_fmax += count_nonzero(f_max)\n",
    "                mhat = m / (1 - beta1 ** t)\n",
    "                state['step_dir'] = mhat / (v.sqrt().add(eps))\n",
    "        use_lr_min = (total_nonzero_fmax < total_nonzero_fmin) #\n",
    "        self.last_total_nonzero_fmin = total_nonzero_fmin\n",
    "        self.last_total_nonzero_fmax = total_nonzero_fmax\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr_min'] if use_lr_min else group['lr_max']\n",
    "            self.last_lr = lr\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None and 'step_dir' in self.state[p]:\n",
    "                    p.add_(self.state[p]['step_dir'], alpha=-lr)\n",
    "        return loss\n",
    "\n",
    "class ASGDAmsgrad(Optimizer):\n",
    "    \"\"\"ASGD-style optimizer mimicking AMSGrad\"\"\"\n",
    "    def __init__(self, params, lr=None, beta1=0.9, beta2=0.999, eps=1e-8,\n",
    "                 lr_min=1e-5, lr_max=3e-4):\n",
    "        defaults = dict(beta1=beta1, beta2=beta2, eps=eps, lr_min=lr_min, lr_max=lr_max)\n",
    "        super().__init__(params, defaults)\n",
    "        self.last_total_nonzero_fmin = 0\n",
    "        self.last_total_nonzero_fmax = 0\n",
    "        self.last_lr = lr_max\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        total_nonzero_fmin, total_nonzero_fmax = 0, 0\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2, eps = group['beta1'], group['beta2'], group['eps']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                g = p.grad\n",
    "                state = self.state[p]\n",
    "                if not state:\n",
    "                    state['t'] = 0\n",
    "                    state['m'] = torch.zeros_like(p)\n",
    "                    state['v'] = torch.zeros_like(p)\n",
    "                    state['v_hat'] = torch.zeros_like(p)\n",
    "                    state['v_prev'] = torch.zeros_like(p)\n",
    "                m, v, v_hat = state['m'], state['v'], state['v_hat']\n",
    "                state['t'] += 1\n",
    "                t = state['t']\n",
    "                m.mul_(beta1).add_(g, alpha=1-beta1)\n",
    "                v.mul_(beta2).addcmul_(g, g, value=1-beta2)\n",
    "                torch.maximum(v_hat, v, out=v_hat)\n",
    "                denom = v_hat.sqrt().add(eps)\n",
    "                dv = v - state['v_prev']\n",
    "                state['v_prev'].copy_(v)\n",
    "                f_min = (dv > 0).to(dtype=torch.int32)\n",
    "                f_max = (dv <= 0).to(dtype=torch.int32)\n",
    "                total_nonzero_fmin += count_nonzero(f_min)\n",
    "                total_nonzero_fmax += count_nonzero(f_max)\n",
    "                mhat = m / (1 - beta1 ** t)\n",
    "                state['step_dir'] = mhat / denom\n",
    "        use_lr_min = (total_nonzero_fmax <  total_nonzero_fmin) # \n",
    "        self.last_total_nonzero_fmin = total_nonzero_fmin\n",
    "        self.last_total_nonzero_fmax = total_nonzero_fmax\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr_min'] if use_lr_min else group['lr_max']\n",
    "            self.last_lr = lr\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None and 'step_dir' in self.state[p]:\n",
    "                    p.add_(self.state[p]['step_dir'], alpha=-lr)\n",
    "        return loss\n",
    "\n",
    "class Padam(Optimizer):\n",
    "    \"\"\"Padam optimizer\"\"\"\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9,0.999), eps=1e-8, weight_decay=0, amsgrad=False, p=0.125):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad, p=p)\n",
    "        super().__init__(params, defaults)\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None: continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Padam does not support sparse gradients')\n",
    "                state = self.state[p]\n",
    "                amsgrad = group['amsgrad']\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                state['step'] += 1\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1-beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                if amsgrad:\n",
    "                    torch.maximum(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "                else:\n",
    "                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "                step_size = group['lr'] / bias_correction1\n",
    "                denom = denom.pow(group['p'])\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(p.data, alpha=group['weight_decay'])\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "# --------------------------\n",
    "# PPO Wrappers\n",
    "# --------------------------\n",
    "class PPOAdamAMSGrad(PPO):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if \"policy_kwargs\" not in kwargs: kwargs[\"policy_kwargs\"] = {}\n",
    "        kwargs[\"policy_kwargs\"].update({\"optimizer_class\": optim.Adam, \"optimizer_kwargs\": {\"amsgrad\": True}})\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "class PPOAdam(PPO):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if \"policy_kwargs\" not in kwargs: kwargs[\"policy_kwargs\"] = {}\n",
    "        kwargs[\"policy_kwargs\"].update({\"optimizer_class\": optim.Adam})\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "class PPOSGD(PPO):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if \"policy_kwargs\" not in kwargs: kwargs[\"policy_kwargs\"] = {}\n",
    "        kwargs[\"policy_kwargs\"].update({\"optimizer_class\": optim.SGD})\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "class PPOPadam(PPO):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if \"policy_kwargs\" not in kwargs: kwargs[\"policy_kwargs\"] = {}\n",
    "        kwargs[\"policy_kwargs\"].update({\"optimizer_class\": Padam, \"optimizer_kwargs\": {\"amsgrad\": True}})\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "class PPOASGDAdam(PPO):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if \"policy_kwargs\" not in kwargs: kwargs[\"policy_kwargs\"] = {}\n",
    "        kwargs[\"policy_kwargs\"].update({\"optimizer_class\": ASGDAdam, \"optimizer_kwargs\": {\"lr_min\":1e-7, \"lr_max\":1e-3}})\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "class PPOASGDAmsgrad(PPO):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if \"policy_kwargs\" not in kwargs: kwargs[\"policy_kwargs\"] = {}\n",
    "        kwargs[\"policy_kwargs\"].update({\"optimizer_class\": ASGDAmsgrad, \"optimizer_kwargs\": {\"lr_min\":1e-7, \"lr_max\":1e-3}})\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# -----------------------------\n",
    "# Training orchestration: train_compare_optimizers\n",
    "# -----------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "def train_and_evaluate_optimizers(modality_choice=\"all\",\n",
    "                                  expression_train=None, pseudotime_train=None,\n",
    "                                  expression_test=None, pseudotime_test=None,\n",
    "                                  modality_splits=None, gene_names=None,\n",
    "                                  adaptive_thresholds=None,\n",
    "                                  optimizers_to_run=None,\n",
    "                                  train_steps=10000,\n",
    "                                  save_dir=\"outputs\"):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    if optimizers_to_run is None:\n",
    "        optimizers_to_run = [\"adam\", \"amsgrad\"]\n",
    "\n",
    "    algo_map = {\n",
    "        \"sgd\": PPOSGD,\n",
    "        \"amsgrad\": PPOAdamAMSGrad,\n",
    "        \"adam\": PPOAdam,\n",
    "        \"padam\": PPOPadam,\n",
    "        \"asgdadam\": PPOASGDAdam,\n",
    "        \"asgdaamsgrad\": PPOASGDAmsgrad,\n",
    "    }\n",
    "\n",
    "    base_kwargs = {\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"clip_range\": 0.2,\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_kwargs\": dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])], activation_fn=nn.Tanh)\n",
    "    }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Modality-specific data\n",
    "    # -----------------------------\n",
    "    if modality_choice == \"all\":\n",
    "        expr_train_mod = expression_train\n",
    "        expr_test_mod = expression_test\n",
    "        pseudo_train_mod = pseudotime_train[next(iter(pseudotime_train.keys()))]  # pick any modality's pseudotime\n",
    "        pseudo_test_mod = pseudotime_test[next(iter(pseudotime_test.keys()))]\n",
    "        gene_names_mod = gene_names\n",
    "        splits_for_env_mod = modality_splits\n",
    "    else:\n",
    "        start, end = modality_splits[modality_choice]\n",
    "        expr_train_mod = expression_train[:, start:end]\n",
    "        expr_test_mod = expression_test[:, start:end]\n",
    "        pseudo_train_mod = pseudotime_train[modality_choice]\n",
    "        pseudo_test_mod = pseudotime_test[modality_choice]\n",
    "        splits_for_env_mod = {modality_choice: (0, end - start)}\n",
    "        gene_names_mod = [f\"{modality_choice}_{i}\" for i in range(end - start)]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Environments\n",
    "    # -----------------------------\n",
    "    train_env = DummyVecEnv([make_env_factory(expr_train_mod, pseudo_train_mod, splits_for_env_mod, adaptive_thresholds)])\n",
    "    eval_env = DummyVecEnv([make_env_factory(expr_test_mod, pseudo_test_mod, splits_for_env_mod, adaptive_thresholds)])\n",
    "    eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "\n",
    "    trained_models = {}\n",
    "    results_reward = {}\n",
    "    train_metrics_df = []\n",
    "    test_metrics_df = []\n",
    "\n",
    "    # -----------------------------\n",
    "    # Training loop\n",
    "    # -----------------------------\n",
    "    for opt_name in optimizers_to_run:\n",
    "        PPOClass = algo_map[opt_name]\n",
    "        print(f\"\\n--- Training {opt_name} on modality={modality_choice} ---\")\n",
    "\n",
    "        model = PPOClass(\"MlpPolicy\", train_env, verbose=1, seed=SEED, **base_kwargs)\n",
    "        model.learn(total_timesteps=train_steps)\n",
    "        trained_models[opt_name] = model\n",
    "\n",
    "        # Evaluate mean reward on test environment\n",
    "        mean_r, std_r = evaluate_model_sb3(model, eval_env, n_eval_episodes=N_EVAL_EPISODES)\n",
    "        results_reward[opt_name] = (mean_r, std_r)\n",
    "        print(f\"Eval mean reward ({opt_name}): {mean_r:.4f} ± {std_r:.4f}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Per-gene evaluation on training data\n",
    "        # -----------------------------\n",
    "        df_train = evaluate_and_plot_multi_modality(\n",
    "            model=model,\n",
    "            algo_name=f\"{opt_name}_train\",\n",
    "            expression_test=expr_train_mod,\n",
    "            adj_matrix=adj_matrix,\n",
    "            pseudotime_test=pseudo_train_mod,\n",
    "            modality_splits=modality_splits,\n",
    "            gene_names=gene_names_mod,\n",
    "            edge_index=edge_index,\n",
    "            edge_weight=edge_weight,\n",
    "            encoder_path=\"gcn_encoder.pth\",\n",
    "            adaptive_thresholds=adaptive_thresholds,\n",
    "            n_episodes=30\n",
    "        )\n",
    "        train_metrics_df.append(df_train)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Per-gene evaluation on test data\n",
    "        # -----------------------------\n",
    "        df_test = evaluate_and_plot_multi_modality(\n",
    "            model=model,\n",
    "            algo_name=f\"{opt_name}_test\",\n",
    "            expression_test=expr_test_mod,\n",
    "            adj_matrix=adj_matrix,\n",
    "            pseudotime_test=pseudo_test_mod,\n",
    "            modality_splits=modality_splits,\n",
    "            gene_names=gene_names_mod,\n",
    "            edge_index=edge_index,\n",
    "            edge_weight=edge_weight,\n",
    "            encoder_path=\"gcn_encoder.pth\",\n",
    "            adaptive_thresholds=adaptive_thresholds,\n",
    "            n_episodes=30\n",
    "        )\n",
    "        test_metrics_df.append(df_test)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save training metrics\n",
    "    # -----------------------------\n",
    "    final_train_df = pd.concat(train_metrics_df, ignore_index=True)\n",
    "    final_train_df.to_csv(os.path.join(save_dir, f\"training_per_gene_metrics_{modality_choice}.csv\"), index=False)\n",
    "    train_summary_df = final_train_df.groupby(\"Algorithm\").mean(numeric_only=True)\n",
    "    train_summary_df.to_csv(os.path.join(save_dir, f\"training_overall_metrics_{modality_choice}.csv\"))\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save testing metrics\n",
    "    # -----------------------------\n",
    "    final_test_df = pd.concat(test_metrics_df, ignore_index=True)\n",
    "    final_test_df.to_csv(os.path.join(save_dir, f\"testing_per_gene_metrics_{modality_choice}.csv\"), index=False)\n",
    "    test_summary_df = final_test_df.groupby(\"Algorithm\").mean(numeric_only=True)\n",
    "    test_summary_df.to_csv(os.path.join(save_dir, f\"testing_overall_metrics_{modality_choice}.csv\"))\n",
    "\n",
    "    print(f\"\\n✅ Training & evaluation complete for all optimizers on modality={modality_choice}.\")\n",
    "\n",
    "    return trained_models, results_reward\n",
    "\n",
    "adaptive_thresholds = PerGeneAdaptiveThreshold(modality_dims)\n",
    "trained_models, rewards = train_and_evaluate_optimizers(\n",
    "    modality_choice=\"rna\",  # or \"all\", \"atac\", \"adt\"\n",
    "    expression_train=fused_train,\n",
    "    pseudotime_train=pseudotime_train,\n",
    "    expression_test=fused_test,\n",
    "    pseudotime_test=pseudotime_test,\n",
    "    modality_splits=modality_splits,\n",
    "    gene_names=selected_gene_names,\n",
    "    adaptive_thresholds=adaptive_thresholds,\n",
    "    optimizers_to_run=[\"adam\",\"amsgrad\"],\n",
    "    train_steps=20000,\n",
    "    save_dir=\"outputs\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df666fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e92a9c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a47ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246bc455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d77a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
